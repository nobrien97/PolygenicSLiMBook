[["introduction.html", "Polygenic SLiMulations: Investigating polygenic adaptation with SLiM 3. 1 Introduction 1.1 Preface 1.2 Overview 1.3 Prerequisites 1.4 References", " Polygenic SLiMulations: Investigating polygenic adaptation with SLiM 3. Nick O’Brien 2021-07-06 1 Introduction 1.1 Preface The past couple of decades have seen the development of powerful computer systems that have become pervasive in every human endeavour. Scientists have long regarded computation as a helpful tool for tackling difficult problems, and leaps in computational power have opened up research opportunities that simply did not exist before. Such is the case with forward-genetics simulation. Although such simulations have existed since the mid-2000s1, their use was limited due to the computational requirements of forward-time simulations being unmatched by the hardware of the time. SLiM changes that, taking advantage of modern computers and an easy-to-learn scripting language that makes forward-genetic simulations relatively straightforward. Combined with a close integration with the msprime coalescent package for python, SLiM is an extremely powerful tool for investigating populations with genetic architectures that are too costly or difficult to analyse numerically. While thorough documentation on introducing biologists to SLiM already exists in the form of the SLiM manual, for those studying complex, polygenic traits, there is a fair amount of trial and error. But fear not, dear reader, for you have stumbled across a shortcut in the form of this document, in which I provide a plethora of templates, tutorials, and tips that I have learned through my time experimenting with SLiM, as well as where to find additional help. 1.2 Overview In this book, I’ll cover: Installation of a Linux environment for Windows 10, Installing SLiM and other useful software (with some fixes for errors I have come across), Using the terminal, basic shortcuts (with hyperlinked video tutorials), Polygenic adaptation resources - a repository of helpful papers to get a feel for quantitative and population genetics What is polygenic adaptation? How is it studied? Population genetics vs quantitative genetics Quantitative genetics models Infinitesimal model Geometrical model Population Genetics theories Neutral theory of molecular evolution Nearly-neutral theory Combining population and quantitative genetics models Extreme value theory Background selection Directional selection vs stabilising selection + balancing selection Hard vs soft sweeps Neutral evolution: Brownian motion and Uhrbeck-Ornstein Linkage Adaptability vs Adaptedness: Effect sizes and mutation rates (HoC vs Gaussian) Modelling polygenic adaptation in SLiM, Template 4: Stabilising selection model of two traits with the above parameters, and a level of pleiotropy between them, and configurable fitness impacts for each Running SLiM in parallel on your computer, Running SLiM on a remote computer (such as a computing cluster or HPC), Using Latin Hypercube Sampling to properly sample a range of genetic parameters, Sorting data, sed, grep for filtering for what you want and statistics for SLiM data in R. Throughout, I’ll put additional information in boxes, so those interested can learn more. Box 1.2.1 Boxes look like this! 1.3 Prerequisites The tutorials in here assume you are at least somewhat familiar with quantitative and population genetics. In addition, you should have completed the SLiM Online Workshop, or at least perused through it to be familiar with the SLiM software. 1.4 References B. Peng, Kimmel M., simuPOP: a forward-time population genetics simulation environment, Bioinformatics, Volume 21, Issue 18, Pages 3686-3687, https://doi.org/10.1093/bioinformatics/bti584↩︎ "],["installing-a-linux-environment-slim-on-the-wsl.html", "2 Installing a Linux environment &amp; SLiM on the WSL 2.1 Overview 2.2 Why Linux? 2.3 The Windows Subsystem for Linux 2.4 Installing WSL 2.5 Installing a desktop and setting up X11 2.6 Building SLiM 2.7 Installing other useful apps 2.8 Footnotes", " 2 Installing a Linux environment &amp; SLiM on the WSL 2.1 Overview SLiM was built primarily with MacOS in mind, so no native Windows version exists. However, a Linux version is around. We can use Windows 10’s new Windows subsystem for Linux (WSL) to run SLiM on Windows (or if we want to be pedantic, we’re running SLiM on a Linux kernel running off Windows). The SLiM manual has a pretty good step-by-step instruction for this if you want to run SLiM by itself, and not worry about using WSL for anything else. However, Linux is a fantastic base for many scientific applications, such as connecting to supercomputers, which are more often than not Linux-based. As such, I’ll be guiding you through installing a desktop environment for WSL, installing R, RStudio, and SLiM, and getting you up to speed with some basics of using Linux. For MacOS users, you can simply install SLiM using the installer package available here. There are also instructions in the SLiM manual for building SLiM from source for MacOS if you would prefer to do that. For Windows users not on Windows 10, I’m afraid there is no way at the moment to run SLiM “natively” on Windows - your best bet would be to dual boot a Linux distribution, or run a virtual machine with VirtualBox or VMware. Box 2.1.1 Linux (or any Unix based operating system, such as MacOS) differs from Windows in the sense that absolutely everything is treated as a file - including devices like printers or monitors. These devices have specialised files that store information about that device. Programs can read those files (or write to them) to communicate with the device itself. Note that these aren’t files in the sense that they are a .txt or something, but they are exposed to the filesystem in the same way and can be treated differently depending on what commands you give it. For example, you could enter cat /proc/cpuinfo or gedit /proc/cpuinfo to print your CPU information to the terminal, or open the file with a text editor so it is readable. You can also use various ‘files’ such as /dev/null or /dev/urandom to instruct other commands. Redirecting output from a command to /dev/null will silence the output so it isn’t displayed in the terminal: slim ~/PolygenicSLiM/box2.1.slim # With output ## // Initial random seed: ## 1731297617719 ## ## // RunInitializeCallbacks(): ## initializeMutationRate(1e-07); ## initializeMutationType(1, 0.5, &quot;f&quot;, 0); ## initializeGenomicElementType(1, m1, 1); ## initializeGenomicElement(g1, 0, 999); ## initializeRecombinationRate(1e-08); ## ## // Starting run at generation &lt;start&gt;: ## 1 ## ## Look at this messy output! slim ~/PolygenicSLiM/box2.1.slim &gt; /dev/null # Output nullified /dev/urandom will generate pseudorandom numbers: od -d /dev/urandom | head # Generate random decimal integers ## 0000000 53834 37313 28164 22638 24519 10509 38065 61564 ## 0000020 28346 9348 52215 47305 11700 58358 46452 41526 ## 0000040 45192 17575 7585 46722 63502 61507 39532 53990 ## 0000060 64752 53704 64565 59033 31105 17965 55951 38873 ## 0000100 34664 40136 58661 50656 52096 37816 64025 36371 ## 0000120 42212 9542 44779 54050 55853 31035 3587 32137 ## 0000140 9468 38228 46581 64876 14483 40201 56641 8433 ## 0000160 49357 36056 28690 18730 22040 4395 56142 58091 ## 0000200 23829 24847 12993 44904 12243 4640 64583 40971 ## 0000220 13596 24421 975 47525 40801 59115 30932 46945 The power of having these very specific commands which can act on a lot of different types of files is that Linux users can chain together these commands to efficiently solve a larger problem. 2.2 Why Linux? To many people, the term ‘Linux user’ is analogous to ‘computer wizard’ or ‘weirdo’. Unfortunately for you, future weirdo, Linux is an incredibly helpful tool for scientific applications. I’ve already mentioned that SLiM doesn’t run on Windows natively, but that is far from the only scientific program that requires (or runs better on) a Linux operating system. Although user-friendly, both Windows and MacOS are extremely bloated, meaning that just running the operating system requires a massive amount of memory and CPU resources that your other applications cannot use. Linux is a barebones system, with the idea being that the user can build the operating system how they want by installing only the features that they will use, saving memory and processor power for the tasks at hand. Another reason for Linux’s speed and low memory footprint is its way of storing files (as mentioned in Box 2.1.1), which is very different to Windows. It takes a bit of getting used to, but in the end you will save massive amounts of time using Linux versus Windows or MacOS. However, there are still programs that you might need to run in Windows because there is no Linux or Mac version. There are three options in this case: Dual-boot by installing a Windows and Linux operating system on separate hard drives (or partitions on one hard drive), Use a traditional virtual machine such as VMware or VirtualBox, Use the Windows 10 Subsystem for Linux (WSL). Dual-booting is great if you rarely use one of your installed operating systems, but generally a bit of a pain if you’re constantly wanting to switch between using Windows and Linux software (you can only run one OS at a time, so you need to reboot if you want to use software in the other OS). VMware and VirtualBox tend to be very resource-intensive, which negates the speed advantage you get from running Linux over Windows. WSL, however, is much faster, and simple to set up. 2.3 The Windows Subsystem for Linux Windows 10 introduced a new way to emulate a Linux machine via a Windows operating system, WSL. The advantage of this approach is accessing your data between operating systems is very easy: your Windows files are visible in Linux, and you can save files to your Windows machine directly using Linux programs. This way, you can use Linux commands such as sed and grep to filter output (extremely quickly) and directly save the output to files that Windows programs can read! Hence, we can run SLiM simulations, storing the output wherever you want, and then use Windows to do data analysis (although you could do that analysis in Linux also). Below, I’ll provide some instructions for installing WSL 2, a Linux distribution (Ubuntu 20.04), and some basics for getting started with Ubuntu. 2.4 Installing WSL There are two choices for running WSL: WSL 1 or WSL 2. Both are fairly similar in speed, however WSL 2 uses a real Linux kernel and an extremely fast virtual machine (along with a virtual hard drive), whereas WSL 1 used a translation layer (instead of a native kernel) to translate between windows and a linux distribution. Ultimately , the main difference is WSL 2 is slightly slower when it comes to copying files between Windows and Linux, but faster nearly everywhere else. Although both versions run SLiM fine2, I’ll be opting for WSL 2. To install WSL, you’ll need to be on Windows 10 version 1903 (build 18362) or newer. To check your version number, run winver.exe. This will tell you the version and build number (build number being in brackets). Instructions for installing WSL 2 can be found on the Microsoft website here. You’ll want to install Ubuntu 20.04 LTS, which can be found on the Microsoft Store (for free). This will install your Linux distribution to the C:/ drive by defualt. If you have limited space on C:/ you can move it to an alternative drive, using this tool. I recommend installing git bash on Windows and running it that way. A side effect of moving your Linux installation is the default user is replaced by root (basically admin for Linux) for some reason, but this is easy enough to fix. In your Ubuntu terminal, open up /etc/wsl.conf with nano and add the following to the end of the file: # Open wsl.conf with the nano text editor, using administrator permissions sudo nano /etc/wsl.conf # Add the following lines to the bottom of wsl.conf [user] default=YOUR_USERNAME If wsl.conf doesn’t exist, create the file manually with sudo touch /etc/wsl.conf. After you’ve added that line, close Ubuntu, then in PowerShell run: wsl -terminate Ubuntu When you next open Ubuntu, you should login as your user again. If that doesn’t work, open up PowerShell and enter: ubuntu config --default-user YOUR_USERNAME Congratulations! You now have a working version of Ubuntu 20.04 LTS running via Windows 10. 2.5 Installing a desktop and setting up X11 At this point you have a Ubuntu terminal which you can use to input commands. This is perfectly usable (even with a desktop you’ll be using the terminal quite a bit), but you’ll probably want a desktop environment to be more comfortable. To do this, enter the following: sudo apt-get update # Refresh Ubuntu&#39;s list of software updates sudo apt-get upgrade # Download and install package updates sudo apt install ubuntu-desktop gnome Box 2.3.1 While we’ll be installing GNOME for this installation, there are a number of other desktop environments you may want to have a look at if you aren’t happy with how GNOME looks or performs on your system. Some others to have a look at are xfce4, Plasma, Unity, MATE, and Budgie. Window managers like dwm or i3 are also good for advanced users, however they are barebones and will likely require more tinkering to get to a form you like. The first two commands will update your base installation and packages; the third will install GNOME (a Linux desktop environment) and relevant dependencies. While those files are downloading, you’ll need to grab a X11 program for Windows. X11 is how Linux draws graphics, so we need that so that when you run GNOME, Linux has a window to draw your desktop. I recommend VcXsrv, which is easily configurable. After downloading and installing VcXsrv and GNOME, there’s a little workaround to get it working on WSL. In the Ubuntu terminal enter the following: sudo nano ~/.bashrc # Open up .bashrc in the nano text editor # .bashrc contains commands that run on startup # Add this one to the bottom of the file in nano and press ctrl+o to save and ctrl+x to exit export LIBGL_ALWAYS_INDIRECT=0 This has something to do with how Linux handles OpenGL graphics drivers, but that’s about as much depth into it as I can go. Without doing this, SLiMgui doesn’t render properly. Next, there’s a setting that needs to be enabled which defaults to disabled in WSL. In WSL 1, this broke a lot of programs (particularly compilers), but it seems to work fine in WSL 2. In Terminal, enter: git clone https://github.com/DamionGans/ubuntu-wsl2-systemd-script.git cd ubuntu-wsl2-systemd-script/ bash ubuntu-wsl2-systemd-script.sh logout Now your Ubuntu window should close. Again, go to PowerShell and terminate Ubuntu: wsl -terminate Ubuntu Next, open up VcXsrv. You should be greeted with a panel that looks something like this: Choose “One large window” and click next, and then next again. At this dialogue, in additional parameters type -ac After pressing next, you should be done! You can save your configuration to create a shortcut that will open VcXsrv with your settings. Click Finish and VcXsrv will open a blank window. Open ubuntu and enter: gnome-session In a few seconds your VcXsrv window should be populated! Each time you start Ubuntu and your X11 server, remember to run gnome-session to enable your desktop. An alternative to this is to create a startup script that automatically runs gnome-session whenever you start Ubuntu. Now you might notice that your desktop doesn’t look exactly like my example above. GNOME is quite customisable, and a plethora of user extensions to personalise your desktop. I won’t cover that here, but I would recommend checking out the GNOME extensions website, and searching around for some GNOME themes. One extension I do recommend installing is nautilus admin, which will allow you to right click in any folder to open a terminal there - a great convenience! sudo apt install nautilus-admin # An alternative if that doesn&#39;t work is sudo apt install nautilus-extension-gnome-terminal 2.6 Building SLiM There’s a few ways of going about building SLiM. We’ll be following the Linux/Unix installation instructions (Chapter 2.2.1) in the SLiM manual. First, install the following packages: sudo apt-get update sudo apt install -y build-essential cmake qt5-default qt5-qmake mesa-utils sudo apt install -y libgl1-mesa-glx x11-apps After this, you should have all the dependencies you need to run the installation script mentioned in the manual: wget -o - --quiet https://raw.githubusercontent.com/MesserLab/SLiM-Extras/master/installation/DebianUbuntuInstall.sh | sudo bash -s This will download and install SLiM, Eidos, and SLiMgui on your system. You should be able to find SLiMgui in your applications now, and you’ll have something looking like this: Box 2.4.1 What are all these dependencies that you’ve installed to install SLiM? Why do I need all that when on Windows you just open an installer? On Linux, a fair few packages are ‘built from source’. This means that the program’s source code is available for download, which means that the user can download that and compile it into an application, while being able to poke around and make adjustments to the software if they want. What the SLiM installer script does is run a series of commands to compile SLiM’s source code into machine code (1s and 0s), which are treated as an application. The applications you installed are the commands which are used to install SLiM: build-essential includes a bunch of compilers to translate human-written code to binary, cmake is a tool to generate makefiles (which are used to tell the compiler how to link certain parts of the code with others, and generally make compiling possible on large projects), qt5 is a graphics framework for drawing windows (such as the SLiMgui window), qmake is a makefile generator specifically for qt projects, and mesa-utils, libgl1-mesa-glx, and x11-apps provide tools to test that X11 and video drivers are working properly. The concept of software compilation is fascinating, so I will leave a link here for anyone interested in learning more. 2.7 Installing other useful apps When using SLiM, you’ll need some more apps to run SLiM on remote computers and to analyse data. I recommend installing FileZilla, which will help copy files across from your computer to a remote supercomputer: sudo apt install filezilla For analysis, I recommend installing R, which is a tiny bit more involved. cd /etc/apt sudo gedit ./sources.list # Add the following line to sources.list deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ Then, in terminal enter the following: sudo apt-get update sudo add-apt-repository &#39;deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/&#39; sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 sudo apt install r-base r-base-core r-recommended r-base-dev This will install the latest release of R 4.0, along with some packages. Next, we can install RStudio: you can choose to install it as an app or as a server, which you can connect to via your internet browser in both Windows and Linux. With some more set up, you could also remotely connect to it via the internet, but I’ll leave that for later. To install RStudio as an app: sudo apt install gdebi-core wget https://download1.rstudio.org/desktop/bionic/amd64/rstudio-1.4.1103-amd64.deb &amp;&amp; sudo gdebi rstudio-1.4.1103-amd64.deb Then you can run rstudio to open the app. To install RStudio as a server app: wget https://download2.rstudio.org/server/bionic/amd64/rstudio-server-1.4.1103-amd64.deb sudo gdebi rstudio-server-1.4.1103-amd64.deb Launch RStudio by running rstudio-server start, and stop the the service with rstudio-server stop. The default settings allow you to connect to the server using whichever browser you’d like with the url localhost:8787. You’ll have to login using your Linux username and password. Note that this server is running on your local machine, so to connect to your server on a different network, you’ll have to do some network configuration (namely port forwarding and setting a static IP). When you have RStudio running (or R if you are just running from the Terminal using R like a madman), you can use install.packages(\"PACKAGE_NAME\") to install packages. Necessary packages will be detailed in later chapters when we get to needing them. To start, try install.packages(\"tidyverse\"). Another nice piece of software is Visual Studio Code. This is a lightweight version of Microsoft’s Visual Studio IDE containing just the text editor. It has really good language tools for a variety of popular languages including Python, R, and C-based languages. For non-SLiM code, this is what I prefer to write in. Another bonus is you can install VS Code for Windows and have it communicate with your WSL installation using the Remote-WSL extension, or you can install it for Linux and run it from in there. Either way, you can download it from the Visual Studio website. If you want to install it on Ubuntu, make sure you download the .deb; you can use: # Make a Installers directory in your /home/ if you don&#39;t have one mkdir ~/Installers cd ~/Installers sudo wget https://code.visualstudio.com/sha/download?build=stable&amp;os=linux-deb-x64 sudo dpkg -i code_* The * is a wildcard that means ‘anything’: so any file that starts with code_ will be selected (which there should only be one of, the installer you just downloaded with wget). dpkg -i is a command to extract and process a debian package, with -i meaning to install it to root (rather than just to the current folder). You can open VS Code without opening a file, using code . When you load VS Code from your WSL Ubuntu installation, you’ll get a message warning you that you should install it on Windows instead. It stills works perfectly fine on Linux, you just have to press Y to accept the message that appears: If you want to install VS Code on Windows and connect remotely to your Ubuntu distribution (you’ll still have access to the Terminal etc.), you can download the installer for Windows from the Visual Studio website and install as usual. From there, you can install the Remote-WSL extension, which will run the VS Code UI on Windows, but all the inner workings on Linux, giving you access to all of the useful tools Linux provides (such as Clang and gcc if you are working with a compiled language). When you work with VS Code, it will automatically detect what language your code is based on the filename. For example, if you save a file as .py, VS Code will ask if you want to install a language extension for Python, which will give you access to autocomplete and error checking a la a full-fat IDE. There is no SLiM/Eidos extension, but it is possible to make custom language extensions. Regardless, this might not be the best way to program for SLiM considering SLiMgui gives immediate, visual feedback on the behaviour of the model by running it in real time and showing mutation frequency dynamics, which VS Code would lack. Congratulations! You now have a Linux desktop, SLiM, RStudio, VSCode and FileZilla installed and ready to go for exploring polygenic adaptation using computer simulations! 2.8 Footnotes One bug I did find with WSL 1 was that Windows Security seems to treat a few programs running on Linux as something suspicious. The result is that Windows Security uses 50% or more CPU while those programs are running. SLiM is included in this, so if for whatever reason you use WSL 1, remember to turn off real-time protection during your SLiM runs and then remember to turn it back on when the run is finished.↩︎ "],["terminal-shortcuts-and-basics.html", "3 Terminal Shortcuts and Basics 3.1 Overview 3.2 Navigating folders 3.3 Installing and updating software 3.4 Input and output 3.5 Git", " 3 Terminal Shortcuts and Basics 3.1 Overview One major difference between Windows/Mac and Linux operating systems is that it’s necessary to learn how to use a command prompt (called the terminal) to use the operating system. You’ve already done a bit of that when you were installing Ubuntu and Gnome, but now I’ll show you some of the most useful commands for navigating directories via the terminal, installing and updating your software, manipulating text files, and what programs you’ll likely use for various tasks. 3.2 Navigating folders When you open a terminal emulator in Gnome, you’ll be met with a screen that looks something like this: Notice the start of the line: this tells you whereabouts in the filesystem you are. By default, when you open a terminal you’ll start at the root directory: the first folder that contains all other folders and files within it. To check your current working directory, type pwd (print working directory). Lets go to our Desktop directory. To do that type cd ~/Desktop. The ~ character tells linux that the directory you are looking for is a branch off of home. Since opening a new terminal defaults you to the home directory, an alternative to do the same thing is ./Desktop. ./ tells linux that the directory is a branch off of the current directory (the one shown with pwd). Now let’s list what’s on our Desktop using the ls command. To go back to your home directory (Desktop’s parent directory), type cd ../. Now lets say we want to create a directory. We can use the mkdir command for that. Go back to your desktop and enter: mkdir Cool_Folder. On your Gnome desktop, you should see that folder appear. If we want to create folders within folders, we can use the recursive flag with mkdir, as follows: mkdir -r Cool_Folder/Cooler_Folder Now say we want to make a folder, and then enter it immediately after. We can do this a number of ways. You could simply call cd as a second command, but you can also use &amp;&amp; to chain together commands, like so: mkdir -r Cool_Folder/Cooler_folder &amp;&amp; cd ./Cool_Folder/Cooler_folder If you ever want to learn about the flags that are available for a certain command, you can enter man command_name to view the manual, or command-name --help to view a usually shorter list of available options. Box 3.2.1 Say you want to make a directory and then enter it immediately rather than having to chain together commands or enter them one at a time. You can do that by creating a custom command and putting it inside your /home/.bashrc file (courtesy of Ouki on Stack Overflow): mkcdir () { mkdir -p -- &quot;$1&quot; &amp;&amp; cd -P -- &quot;$1&quot; } 3.3 Installing and updating software With Ubuntu, you don’t need to worry about Windows updates taking your computer hostage whenever it feels like it. However, it is on you to keep your software updated (or not, if you need a particular version of some software!). To update all the packages you have installed, run the following: sudo apt-get update sudo apt-get upgrade sudo is like ‘Run as Administrator’ in Windows, and allows programs to gain root access (access to non-user directories like where most of your software is installed). apt-get update updates the list of Ubuntu-supported packages you are able to install directly via Ubuntu. apt-get upgrade downloads those latest versions of the Ubuntu-supported packages you have installed and updates them for you. Before installing new software using apt-get, it’s always a good idea to run apt-get update to make sure you’re downloading the latest version. Now lets say we want to download a new program - like fortune, a little program that shows a quote on the screen. sudo apt-get update sudo apt-get install fortune-mod If you don’t know the full name of a package (or how to spell it), you can press tab to autocomplete your line, or double-press tab to give a list of all the possible packages available matching what you’ve typed so far. This works for anything in terminal, and is extremely useful, especially when cd’ing to a directory: rather than typing out the whole directory, you can usually hit tab a few times, type a few letters and hit tab some more to get to where you want. Much faster! apt-get install also works with multiple packages at once. For example, sudo apt-get update sudo apt-get install fortune-mod cowsay This will install both fortune-mod and cowsay (a program that draws an ASCII animal with a speech bubble saying whatever you type). Now that your programs are installing, you can try them out! fortune cowsay &quot;I don&#39;t think, therefore I don&#39;t am.&quot; ## Q: How do you shoot a blue elephant? ## A: With a blue-elephant gun. ## ## Q: How do you shoot a pink elephant? ## A: Twist its trunk until it turns blue, then shoot it with ## a blue-elephant gun. ## ______________________________________ ## &lt; I don&#39;t think, therefore I don&#39;t am. &gt; ## -------------------------------------- ## \\ ^__^ ## \\ (oo)\\_______ ## (__)\\ )\\/\\ ## ||----w | ## || || Box 3.3.1 As I’ve alluded to with the tab shortcut, the Linux terminal has many useful shortcuts that can save a lot of time. I’ll describe a few below: Shortcut Function Ctrl + Alt + T Open a new terminal Ctrl + C Cancel the current command Ctrl + L Clear the terminal of all output Home Move to the beginning of the line End Move to the end of a line Ctrl + U Clear the line Ctrl + Shift + - Undo Ctrl + Shift + C Copy Ctrl + Shift + V Paste Ctrl + R Recall the last command you entered with the characters you provide ! Run the last command again Similarly, rather than typing out entire directories every time, you can use some special characters to get some of the way there: Character Location Example ~ Home directory ~/Desktop . Current directory ./ChildDirectoryofCurrentDirectory .. Parent directory ../build / Root directory /bin And here are a number of some really helpful commands that you’ll likely be using often. I’ll only be showing you the base cases here: often there are a number of options you can enable to do different things (e.g. recursively make directories with mkdir -r), but to see the available options all you need to do is look for the command’s manual page with man &lt;command&gt; Command Description Example cat Concatenante/display a file’s contents in terminal cat ~/.bashrc cp Copy a file/folder to another folder cp ~/file.txt ~/Desktop mv Move a file/folder to another folder mv ~/file.txt ~/Desktop rm Remove a file/folder (delete) rm ~/file.txt alias Assign a string of commands another command name alias cowfort=“fortune | cowsay” curl Fetch a file from the internet (not pre-installed; alternative to wget) curl https://github.com/nobrien97/PolygenicSLiMBook/blob/main/src/SLiM/box2.1.slim -o box2.1.slim grep Search a file’s lines for a particular pattern grep hello ~/file.txt sed Modify text coming in from stdin. Combine with grep for powerful editing See here top Display info on active processes, CPU and memory usage etc. top head Display the first few lines of a file head ~/multilinefile.txt tail Display the last few lines of a file tail ~/multilinefile.txt tar Bundle files into an archive (compressed with -j or -z) tar -cg Files.tar.gz ~/FolderOfFiles ssh Connect to a remote server ssh user@tinaroo1.rcc.uq.edu.au shutdown Shuts the Linux system down in one minute (now for immediate) shutdown now less Open and read a file (press q to exit) less ~/file.txt vim Powerful terminal text editor with steep learning curve vim ~/file.txt nano Considerably more user friendly text editor nano ~/file.txt gedit Another text editor, this time exists out of the terminal gedit ~/file.txt history Display history of commands in the terminal. Rerun any of them with !CommandNumber history &amp;&amp; !! &amp;&amp; !3 3.4 Input and output Linux programs are written in such a way that programs take their inputs and spit out their outputs in a standardised way. This means that you can use the same command or tool to affect many different programs’ outputs or inputs with the same syntax. This is done through input and output ‘channels’. The main ones are standard input (stdin), standard output (stdout), and standard error (stderr). You might also see these referred to as file descriptors. Programmers can write code to send their output to stdout (and error messages to stderr) to avoid having to implement their own system. If they require addition functionality, they can create their own channels as well. For more on input/output and file descriptors, see this video. By default, stdin takes the input of what the user types in the terminal, and stdout (and stderr) prints everything to the terminal. This behaviour can be changed by using pipes and redirection, making these input/output streams very powerful indeed. One consequence of this is we can chain together commands, feeding their output into another command. Take this example: fortune | cowsay ## _________________________________________ ## / No live organism can continue for long \\ ## | to exist sanely under conditions of | ## | absolute reality; even larks and | ## | katydids are supposed, by some, to | ## | dream. Hill House, not sane, stood by | ## | itself against its hills, holding | ## | darkness within; it had stood so for | ## | eighty years and might stand for eighty | ## | more. Within, walls continued upright, | ## | bricks met neatly, floors were firm, | ## | and doors were sensibly shut; silence | ## | lay steadily against the wood and stone | ## | of Hill House, and whatever walked | ## | there, walked alone. | ## | | ## | -- Shirley Jackson, &quot;The Haunting of | ## \\ Hill House&quot; / ## ----------------------------------------- ## \\ ^__^ ## \\ (oo)\\_______ ## (__)\\ )\\/\\ ## ||----w | ## || || Here, we use the |, called the ‘pipe’, to ‘pipe in’ fortune’s stdout into cowsay’s stdin. The result is a cow with a textbox (cowsay) relaying a quote from fortune. You can also save the output of a command to a file using redirection &gt; Take this example: echo &quot;Hello, world!&quot; &gt; ~/Desktop/file.txt Redirection is in particular very important in general, and will be used pretty extensively to copy files to the proper place, append files with new lines of results from SLiM, and interface with supercomputers (all coming later!). For that reason, I strongly suggest watching this 10 minute video on all the common redirection and piping methods available so when you see it in future (or need to use their functionality!) you know what you’re looking at. 3.5 Git Git is a version control software which allows you to maintain a working history of your files. It’s most useful with plaintext files, such as code, where you can directly view the differences between an older version of a file and the current version. In large projects it is vital, but even in small projects with only a few files, version control is very important for organisation, and being able to keep track of your work easily. If you want to test out a change but you are unsure if it will work, Git allows you to do that while always being able to go back to your old version if required. You could save a backup file somewhere, but this is clunky, prone to error, and hard to maintain over time. Git can be a bit confusing to start out, but because of its ubiquity there is a massive amount of information available. Simply googling what you are trying to do will likely get you in the right direction. I would also highly recommend following GitHub’s tutorial to create your first repository, and also checking out Atlassian’s git tutorials. To save your data online, Git has a feature called ‘remotes’. You can have a local git repository/folder to keep track of your files, and then a remote repository that you and/or others can access from the internet. You can update your files locally, and when you want to update the version on the cloud you ‘push’ them to your remote repository. There a few services which host remotes, but the most used is GitHub. Git is a command line tool, however it is directly integrated into a variety of software. Both RStudio and VS Code have git support, for instance. Because of this, you will rarely have to use the command line to use git. Nonetheless, there are a few command line options you should know about in case something isn’t working for you with these programs’ integration. # Make a new Repository mkdir ~/MyRepositoryFolder cd ~/MyRepositoryFolder git init MyRepository git add * # add all files that git init generates to the repository git commit -a -m &#39;Initial commit&#39; # commit all files to the repo with message &#39;Initial Commit&#39; # Link a repository to a remote (create a new remote at [GitHub](https://github.com/new)) git remote add origin https:/github.com/UserName/MyRepository.git # Clone a repository (easiest way to make a new repo: create on GitHub, then clone that into a local folder) git clone MyRepository https:/github.com/UserName/MyRepository.git # Add edited/new files to your repository git add . # &#39;.&#39; to add all edited files in the folder, you can alternatively specify particular files git commit # This will open up a text editor asking for your commit message git push # This will push your commit to the remote # Remove files from the repository, recursively and forcefully git rm -rf ./AwfulFile.c git commit In VS Code,the sidebar has a ‘Source Control’ tab that will allow you to link a repository and/or a remote. If you load VS Code from within a repository, i.e. cd ~/MyRepository; code ., it will do this automatically, assuming you already have a remote linked. This tab will keep track of the changes you’ve made in the local repo, with options to ‘stage changes’ (git add), commit, and push to a remote, although the remote is hidden by default and will require you to right click on the ‘Source Control’ header and tick the ‘Source Control Repositories’ flag: You might find that some files won’t automatically add themselves to the ‘Changes’ list in VS Code, in which case you’ll have to click the ‘Refresh’ button next to ‘Source Control’. If this doesn’t work, you’ll have to do it in command line. I find that VS Code is generally good at keep track of existing files, but not very good at keep track of the folder itself: that is, removing files and adding files generally won’t be picked up by VS Code, so you’ll need to directly add them through the command line with git add .. 3.5.1 Git Branches and Pull Requests Sometimes you’ll want to change a heap of files at once in your git repository, and going back to the original state if it doesn’t work would be a massive pain. Alternatively, you might want to have a separate files organised in a separate ‘partition’ of your repository. You might also be collaborating with others, in which case you don’t want to be constantly overwriting each others’ work (although Git does automatically merge files with changes on different lines, if you are editing the same file on the same line, some decisions need to be made as to which version to keep). This is where branches come in. A branch is a splitting off point from your repository. You can imagine your repository as a phylogeny: over time, the files change, and branches are created at various points in time creating new, distinct edits. The idea is you can preserve your main repository’s files, work on an experimental branch, and then either abandon it if it doesn’t work, or merge it into your main branch using a ‘pull request’. If you’re collaborating, you can create your own branch so as not to disturb other collaborators, and when you are done, you can create a pull request to pull your branch into the main one. You can also use branches to organise your repository into categories: for example, R bookdown repositories often have a separate branch for hosting the .html website, and another for storing markdown documents, uncompiled code, and data not relevant to the bookdown itself. From https://www.atlassian.com/git/tutorials/using-branches To create a new local branch and link it to a new remote branch, you can use the following commands: git branch NewBranch # Create the new local branch git checkout NewBranch # Swap to the new local branch git push NewRemoteBranch NewBranch~ # Create a new branch on the remote called NewRemoteBranch To view all of the branches available on your project, you can run git branch with no arguments. Most of the time, branches aren’t super necessary. They do have their uses, but I wouldn’t recommend creating a heap of branches for every experiment you want to do: you can always use git reset to reset your repo to a certain commit. Similarly, pull requests are only necessary if you’re collaborating and need to have code approved. For more information on that, I’d recommend checking out Atlassian. 3.5.2 Git for SLiM I find Git very helpful for maintaining SLiM code. Particularly when playing around with input variables and adding new features, it can be useful to keep track of what is made. However, the main feature I like is the cloud backup which is always available at GitHub. "],["placeholder-chapter.html", "4 Placeholder Chapter", " 4 Placeholder Chapter This will be replaced by something else. In the mean time, enjoy this cat picture: "],["polygenic-adaptation-in-slim.html", "5 Polygenic Adaptation in SLiM 5.1 Overview 5.2 A Single Polygenic Trait (Chp5-1_1T.slim) 5.3 Parameters", " 5 Polygenic Adaptation in SLiM 5.1 Overview In this chapter I’ll provide a number of templates aiming to reconstruct some of the polygenic models we have seen in SLiM. I’ve aimed to make these templates highly customisable, with many features that can be adjusted as you see fit. As such, I’ll break down each section of the code to explain what it does, which may give you ideas for how it could be extended for your own use cases, or which parts can be removed if you don’t need them and want to maximise your speed and/or minimise memory use. 5.2 A Single Polygenic Trait (Chp5-1_1T.slim) The first example we’ll go over models a single polygenic trait evolving through either drift, stabilising selection, or directional selection. This template has been built to be very broad, with a large array of parameters. These parameters are defined at the top of the file - any line beginning with SetCfgParam() is a different parameter you may adjust\\(^*\\). The defaults are set based on realistic values, with sources cited in comments. See Box 5.2.1 for information on each of these parameters. \\(^*\\) : SetCfgParam() is based on the SLiM function defineConstant(), but works for both command line and SLiMgui so no changes are necessary to switch between the two. The model begins with a specified number of generations of burn-in (given via the parameter burnTime). This burn-in period involves mutations subject to neutral drift, which yields standing genetic variation for selection to act on when a fitness function is applied. The idea is for the population to be at mutation-drift equilibrium by the time the selection regime is applied. I’ve found this typically occurs within 50,000 generations for a population size of 8,000, but you’ll likely want to adjust this as you see fit. One way to confirm when a population is at equilibrium is by tracking heterozygosity over time. In the SLiM script (Chp5-1_1T.slim) you can do this by setting SetCfgParam(printH, T); (Line 36). This will plot heterozygosity over time during your set burn-in period for you to view. When the population is at equilibrium, this value should remain relatively stable around \\(\\theta = 4N_e\\mu\\), however the amount of stability around this value (or any value at all) is dependent on several parameters including linkage (given by rwide), and background selection (determined by del_mean, del_shape, and `mutWeights). During burn-in, a variety of output files are generated. These include files containing - Information on which genome positions are QTLs as opposed to non-QTL sites - Heterozygosity and phenotypic means over time (sampled every X generations, defined by the first value in samplerate). - The phenotypic location of the trait optimum. Following burn-in, a certain selection regime takes place for a specified number of generations. This is set using the selType and testTime parameters. Here, the population follows a stabilising or directinal selection trajectory, or continues via neutral drift (“Brownian motion”). During this process, some more output is generated: - Information on heterozygosity, phenotypic means, and fitness (sampled at a rate fiven by the first value in samplerate) - Allelic effect sizes, frequencies, fixation times, etc. (sampled at a rate given by the second value in samplerate) Note that mutation output is quite large (seeing as it stores many lines every time it writes to file), so you will want to adjust the sampling frequency as per your available storage, the size of your simulation (in number of generations, size of the genome, number of mutations/mutation rate etc.). 5.3 Parameters The script template contains a lot of parameters. Some of these will be applicable to you, others won’t be. I’ll explain each of them here so you can get an idea. In brackets next to the parameter name is the type it expects, and how many arguments it expects (if &gt; 1). Parameter (type) Description mu (float) Genome-wide mutation rate Ne (int) Population size del_mean and del_shape (float) Mean and shape parameters used to pull effects of deleterious mutations mutWeights (float, 3) Proportions of mutations of different types rwide (float) Genome wide recombination rate nloci (int) The number of QTLs to model genomelength (int) The total length of the genome in loci locimu, locisigma, locidist (float, float, string) Parameters for QTL distribution of additive effects. width (float) Stabilising selection fitness function width parameter opt (numeric) Position of the optimum relative to the burn-in phenotype mean selType (string) The type of selection to use during the test: “s” = stabilising, “p” = positive/directional, “d” = drift printH (bool) Draw a graph of heterozygosity during burn-in to diagnose mutation-drift equilibrium burnTime (int) Number of generations to run neutral burn-in for. testTime (int) Number of generations to run the selective regime for. samplerate (int, 2) How often, in generations, to save output out… (string) Output filenames and paths for various output files modelindex (int) Identifier for Latin Hypercube job arrays Below are some tips regarding some of the parameters to guide you in designing your experiments: 5.3.1 Ne The single most expensive thing to do in SLiM is increase the population size. Be careful with setting this and time your simulations as you adjust it so you can maximise your population size for the available time you have to run your simulations. Be aware that smaller population sizes = stronger drift, and so selection will be comparatively weaker in small populations. 5.3.2 del_mean and del_shape When mutations appear in SLiM, a fitness effect is pulled from a user-specified distribution of fitness effects. Individuals with the mutation have their fitness adjusted according to this effect. Non-trait deleterious mutations are pulled from a gamma distribution in this simulation, which asks for a mean and shape parameter. This is slightly different to the standard mathematical notation, which usually specifies a \\(\\alpha\\) and \\(\\beta\\) parameter. In this case, the mean is equal to \\(\\alpha\\beta\\), and the shape parameter is \\(\\alpha\\). You are able to see the shape of your distribution of effects for any mutation type by clicking the hand underneath the generation counter in SLiMgui and hovering over the mutation type: 5.3.3 mutWeights In this simulation, not all genome positions are QTLs. Some are sites with nothing to do with the trait whatsoever. Therefore, there are 3 mutation types: neutral (m1), which do nothing except give an idea of how linked variation declines with other types; non-trait deleterious (m2), which act as a form of background selection: imagine these as being mutations that don’t affect the trait but reduce fitness; and QTLs (m3), which affect the trait. All three of these types can occur at a QTL site, but QTL mutations (m3) cannot occur at non-QTL sites. Imagine a mutation occurs at a QTL that causes a pleiotropic effect elsewhere, resulting in an extremely deleterious effect. This is the kind of situation this is modelling. Similarly, a neutral mutation could occur at a QTL site that is synonymous, not changing the phenotype at all. In this simulation, all mutations begin life as an m1 mutation. The simulation checks if any mutation exists at a QTL site and then randomly chooses a type based on mutWeights. This parameter specifies the relative likelihood of a given mutation type (neutral vs QTL vs deleterious) occurring. 5.3.4 rwide From my experience, recombination rate in SLiM doesn’t typically affect performance too much when using realistic values. However, if you want to model independent assortment (by setting rwide to 0.5) to get rid of linkage effects, you will probably see a tiny\\(^1\\) performance drop\\(^2\\). I’d suggest testing this out, and if you want to model independent assortment, do so with a small genome (or a small population). This parameter is set as a single genome-wide value, however recombination maps can be constructed in SLiM using a vector of rates followed by a vector of endpoints where the next rate begins. This will require some small adjustments to the script, but should be trivial given you read the documentation on the initializeRecombinationRate() method. \\(^1\\): Massive \\(^2\\): Death of your computer 5.3.5 genomelength This simulation assumes each ‘box’ in the SLiM genome is a locus rather than a base pair. This means all rates (rwide, mu etc.) are scaled according to per locus per generation, rather than per base pair. nloci specifies the number of QTL positions to be randomly scattered across the genome, whereas genomelength is the actual genome length in total loci. Hence, nloci must be \\(&lt;=\\) genomelength. 5.3.6 locimu, locisigma, and locidist QTL mutations effects are pulled from a normal distribution by default in this simulation, with mean = locimu and variance = locisigma. This can be changed though, using the locidist parameter. A normal distribution (default) is set using “n”. A gamma distribution of effects can also be chosen using “g”. 5.3.7 width The fitness function being used for stabilising selection comes from Lande 1976 (SOURCE). This equation comes out to the form \\(f(z)\\ =\\ 0.01+\\exp(-(z^{2})*\\left(\\frac{1}{2w^{2}}\\right))\\) Where f(z) is the fitness of an individual z units from the optimum, w is the width of the function, and z is the Euclidean distance from the optimum. You can play around with that here. Moving the width will show you how fitness decays as an individual gets far from the optimum: higher width means weaker selection (lower difference between individuals at the optimum vs those far away). The width parameter is precomputed to save a little bit of computing time - computers don’t like doing division very much, and doing that every time we calculate fitness is wasteful. Hence, we precompute \\(\\frac{1}{2w^{2}}\\), and that is our width parameter. 5.3.8 printH Setting this option to T will enable a diagnostic plot of heterozygosity over time to help visualise mutation-drift equilibrium for your given mutation rate and population size (see Section 5.2). Dotted lines drawn indicate 10% CIs of \\(\\theta\\), a measure of nucleotide diversity given by \\(\\theta = 4N_e\\mu\\). 5.3.9 samplerate There are two values set in the samplerate parameter. The first value corresponds to ordinary outputs (means, heterozygosities etc.), which occur during burn-in and the test. The second value corresponds to mutation output, which is considerably large due to printing many lines per generation. As a result, you likely will need to sample mutations less frequently than other output in order to save space (and have your files openable in R without running out of memory). 5.3.10 modelindex modelindex is an important parameter for Latin Hypercube sets of simulations, however we’ll get into that later. For now, you can use it individually to quickly identify different simulations where you have changed a bunch of parameters (e.g. if you have three levels of rwide, you could set three different modelindex values to identify those models. This is more useful when combined with interactions between multiple parameters). It can also be used as a proxy for all of your variables, saving considerable space when storing output. For example, you will only need one column to identify the parameters which led to a result rather than a column for each parameter. This is particularly input with regards to mutation output, which grows to potentially 100s of gigabytes in size. "],["running-slim-in-parallel.html", "6 Running SLiM in Parallel 6.1 Overview 6.2 SLiM at the Command Line 6.3 Running SLiM via Bash 6.4 Running SLiM via R 6.5 Running SLiM in Python 6.6 Running SLiM via a C++ Program 6.7 Writing SLiM code with parallelism in mind 6.8 Footnotes", " 6 Running SLiM in Parallel 6.1 Overview Now that we have an understanding of SLiM and a couple of models that might be useful, you’re probably wondering: how do we make this useful? To answer a question, we need multiple simulations with different variable inputs (e.g. different recombination rates), and replicates of each. The problem here is that the more simulations you need to do, the longer the time required if you were to run them sequentially. Luckily, SLiM is designed with parallelism in mind. That is, you can have many SLiM simulations going at once, each running on a separate CPU core. This can all be done in whatever programming language you like, as long as it has a function to invoke system operating system commands (like system() in R), and support for multicore processing. For more on parallelism, I’d recommend reading this crass, but entertaining guide, particularly the first few sections. This will get you up to speed on the strengths and limitations of parallel processing. In this section, I’ll go over how to run SLiM at the command line in a number of languages, and how to parallelise in each of them. I even provide a C++ implementation which is overkill and not necessary at all, but it was fun3 to program! Note that more information (and indeed some of the scripts you will see below are based upon these) is available at the SLiM-Extras repository under ‘sublaunching’. Source code for all of the examples shown in this document is available in the /src/Parallelisation folder. 6.2 SLiM at the Command Line To use SLiM from the command line, you simply call the slim command, followed by a path to a file. For example: slim ~/Desktop/example_script.slim This would run the script ‘example_script.slim’ with it’s set parameters. Say you want to do some replicates. To do this you might want to change the random seed, which adjusts state of the random number generator that determines when, where, and how mutations, recombination, and mating between certain individuals might happen. You can do this using the -s flag: slim -s 123 ~/Desktop/example_script.slim This would run the above script with the seed 123. Note that you shouldn’t choose numbers like this, but instead use a random number generator to generate random numbers for you. Similarly, you can adjust parameters in your model with the -d flag, of which you can have as many as you want. For example, say you want to change the population size which is defined in your script as a parameter called Ne. You could run: slim -s 123 -d Ne=1000 ~/Desktop/example_script.slim Here, we’ve set Ne to be 1000, so slim will run the script with 1000 individuals and a seed of 123. Box 6.2.1 When it comes to random numbers, it comes as little surprise that humans are bad at generating them. We have biases towards certain numbers by conscious (or unconscious!) recognition of those numbers. But it turns out that computers have just as hard a time as we do! Computers are designed to be deterministic, so generating random numbers can be quite difficult. To do it, various algorithms have been written to generate numbers from some distribution, fed with a random ‘seed’ to spit out that number. This seed is typically a signed 32-bit integer (of the range -2,147,483,648 to 2,147,483,648), but realistically any size or type can be used (and 64-bit integers are becoming increasingly common as the range of possible values is massive). But how do you generate a seed? With another random number generator. Which must have come from some deterministic process. So no matter how many generators you have generating seeds for your random output, you can always go back to a deterministic value that seeded it all. It isn’t ever truly random, just pseudo-random. Of course, certain trickery has allowed for more complex randomness: some of the latest has to do with quantum mechanics of a laser reflecting or absorbing photons and treating the output of the number absorbed vs reflected as a random number, which is theoretically truly random. But do you really need a laser to seed a pseudo-random process? Or a system of lava lamps (a la CloudFlare)? Probably not. As long as you are blind to the original seed that created your random numbers, it might as well be random: the chance that you are going to be able to find that original seed is astronomically small (\\(\\frac{1}{2^{63}-1}\\) for a signed 64-bit seed). /dev/random, which we saw in Box 2.1.1, generates random numbers based on environmental noise from your computer hardware drivers, which is supposedly close to ‘true’ randomness. But is ‘true’ randomness a thing at all? Is the universe deterministic? What is the universe’s seed? If you put the universe’s seed into a Minecraft world generator, would that world have many diamonds? Food for thought. The point is that you should be careful with generating random numbers and make sure you know that the ultimate source of randomness is as close to ‘true’ random as possible, but if it isn’t, as long as it is secure and probabilistically close to impossible to crack, it is probably a good enough source of randomness for whatever you are trying to do. OK, so we can run SLiM via the command line for a single seed or variable combination. Now we could run that command multiple times, changing the seed or parameters manually every time we run it, but that’s inefficient and makes it difficult to use all your available resources on your computer (notably, cores for running separate SLiM instances in parallel). This is where sublaunching scripts can come in handy. These enable us to use a pre-written script to vary all the parameters we would like automatically, and run many SLiM experiments at once as possible. As a quick aside, these template scripts I’m providing require a seeds.csv file to be in the format of a single column with a header called ‘Seed’. My SeedGenerator program which I have provided (Under “Tools” in the main GitHub branch), (along with an install script) will generate these for you. To use SeedGenerator, simply run seedgen_install.sh, and then ./seedgenerator. There are a variety of options, listed with the -h or –help flag: # Shows the help file with instructions on each of the options ../Tools/SeedGenerator/seedgenerator --help ## Uniformly Distributed Seed Generator ## ## This program generates a .csv of uniformly distributed 32-bit integers for use as RNG seeds. ## Usage: ../Tools/SeedGenerator/seedgenerator [OPTION]... ## Example: ../Tools/SeedGenerator/seedgenerator -h ## ## -h Print this help manual. ## ## -n N Generate N random samples. Defaults to 10. ## ## -v Turn on verbose mode. ## ## -t NAME Choose a header name. Defaults to &#39;Seed&#39;. Enter nothing to have no header. ## Example: -t=Number OR -tNumber ## ## -d FILEPATH Specify a filepath and name for the generated seeds to be saved. Defaults to ./seeds.csv. ## Example: -d ~/Desktop/seeds.csv Now onto sublaunching SLiM. We’ll start with Bash, which is perhaps the simplest. 6.3 Running SLiM via Bash The simplest way to run SLiM at the command line is through Bash, the standard command line scripting language of Linux based systems. Note that the SLiM Online Workshop - ‘Running SLiM from the command line’ tutorial goes over much of what I’ll be presenting below. The Bash syntax is pretty similar to the standard way of running SLiM on the command line, and in fact you’ll be using almost identical commands. for seed in seeds.csv; do echo &quot;Running with seed == &quot; $(seed): slim -s $(seed) ~/Desktop/example_script.slim &amp; echo done Here, we do a simple for loop over the seeds in a file called seeds.csv (generated by Tools/SeedGenerator/seedgenerator). For this to work in bash, make sure you have the header disabled (./seedgenerator -t) The &amp; character tells Bash to run the slim process as a background task, meaning it is put on an available core. This results in SLiM processes running parallelised across multiple cores! Bash can also be used to parallelise over many parameters, but it quickly becomes difficult to read. For example, here is an example script with two parameters and a seed variable: for param1 in 0.1 0.2 0.3 do for param2 in &quot;Low&quot; &quot;Medium&quot; &quot;High&quot; do for seed in seeds.csv do echo &quot;Seed = &quot; $(seed) &quot; param1 = &quot; $(param1) &quot; param2 = &quot; $(param2): slim -s $(seed) -d param1=$(param1) -d param2=$(param2) ~/Desktop/example_script.slim &amp; echo done done done While it is also possible to read in files and split columns into different variables, it’s much easier to do this in R or Python. So for more complex scripts, I would highly suggest using either of those to sublaunch your SLiM jobs. I will provide examples of both below. 6.4 Running SLiM via R This is how I normally sublaunch SLiM. R has a variety of packages that make it easy to parallelise across cores, and because of R’s rich feature set and the breadth of user-made libraries, the options are endless when it comes to feeding parameters to SLiM, loading output, or integrating with SLiM. # Parallelisation libraries library(foreach) library(doParallel) library(future) seeds &lt;- read.csv(&quot;~/Desktop/seeds.csv&quot;, header = T) cl &lt;- makeCluster(future::availableCores()) registerDoParallel(cl) #Run SLiM foreach(i=seeds$Seed) %dopar% { # Use string manipulation functions to configure the command line args, feeding from a data frame of seeds # then run SLiM with system(), slim_out &lt;- system(sprintf(&quot;/path/to/slim -s %s ~/Desktop/example_script.slim&quot;, as.character(i), intern=T)) } stopCluster(cl) This script first loads a series of libraries that allow R to run a for loop across multiple cores. Each iteration of the for loop is assigned to a free core when it becomes available. We then load in seeds.csv as a dataframe, and set up a local ‘cluster’, which basically lets R know how many cores are available on your system that it can use. The seeds are then fed into our for loop (foreach(), which is implemented specifically to be parallel). For each seed, we run the for loop, with the %dopar% operator saying that we should do those iterations across as many cores as are available. The system() command tells the operating system to run a command, given as a string. sprintf() creates a string from its inputs, with support for variables to be added to the string on the fly. This is done with the % symbol followed by the type that is being fed to sprintf(). For example, %s provides a placeholder for a string variable. These variables are listed at the end of the line _in the order that they are mentioned in the sprintf() command. For example, here %s, being the first variable mentioned in the string, is replaced by the first variable mentioned after the string ends with the closing \". In this case, we feed SLiM a seed with the -s command, replacing %s with as.character(i), which is the seed given by the for loop. As well as %s, there is also i and f for integer and floating point (or double, since R only supports doubles) numbers. So why do we feed SLiM a string as a seed instead of an integer? It’s to ensure it gets read properly. R only supports signed 32-bit integers, which are a fair bit smaller than SLiM’s 64-bit integers. Since the idea of randomly choosing seeds is to uniformly sample across the entire range of possible values to avoid any kind of correlations, we sample across the range of 64 bit values. However, R can’t handle numbers so big as integers, so it automatically coerces them to doubles. Doubles are stored in computers very differently to integers, and having R treat this number as an integer (or as a float) and then feed it to SLiM that way results in unexpected behaviour. I’ve found it’s safest to just treat the seed as a string so no coercion happens - SLiM automatically will treat that string as an integer when it is loaded anyway. Now, the R script is a little more complex at base-level than a Bash script, but I find it much easier to expand upon. Say for example we have two parameters as before. We could use nested foreach loops just like we did in Bash, but it’s easier to exploit R’s dataframe support to have a dataframe of possible parameter combinations, and then use each iteration to choose the right combination. This way, we simply need one level of nesting regardless of how many parameters we have: one foreach loop for seeds, and a nested one for parameter combinations. # Create a list of parameters p &lt;- list() p$param1 &lt;- c(0.1, 0.2, 0.3) p$param2 &lt;- c(&#39;&quot;Low&quot;&#39;, &#39;&quot;Medium&quot;&#39;, &#39;&quot;High&quot;&#39;) #&#39;&quot; is necessary for SLiM to read them as strings # Save the list as a data frame with all possible combinations df.p &lt;- expand.grid(p) # You can also save df.p as a csv file and import it later, as with seeds: write.csv(df.p, row.names = F) # Now we can use those data frame rows as inputs for our script # Parallelisation libraries library(foreach) library(doParallel) library(future) seeds &lt;- read.csv(&quot;~/Desktop/seeds.csv&quot;, header = T) cl &lt;- makeCluster(future::availableCores()) registerDoParallel(cl) #Run SLiM foreach(i=1:nrow(df.p)) %:% foreach(j=seeds$Seed) %dopar% { slim_out &lt;- system(sprintf(&quot;/path/to/slim -s %s -d param1=%f -d param2=%s ~/Desktop/example_script.slim&quot;, as.character(j), df.p[i,]$param1, df.p[i,]$param2, intern=T)) } stopCluster(cl) Here we do the exact thing as before: creating a local cluster and running a foreach loop. However, in this case we nest a second foreach loop so we can include both seeds and the parameter combinations. Each i in this loop is a different row in the df.p dataframe of parameter combinations. Each j is a different seed. Notice we only ues %dopar% once, and use that on the innermost foreach loop. This means for each parameter combination (i), we will parallelise across seeds (j). This can be extended to as many parameter values as you want, as we simply fill each parameter using the sprintf() variable-filling functionality as before, this time referencing df.p and choosing the appropriate column (parameter value). 6.5 Running SLiM in Python Running SLiM in Python is similarly straightforward, and due to its plethora of libraries, quite powerful also. Here’s an example from os import system from multiprocessing import Pool from pandas import read_csv # Open the seeds file seeds = read_csv(r&#39;../Inputs/seeds.csv&#39;) def slim_call(seed): system(&#39;slim -s {} ~/Desktop/example_script.slim&#39;.format(seed)) # Open a new &#39;pool&#39; - like makeCluster() in R cluster = Pool() # Do an operation on the pool - this is like mclapply() in R if __name__ == &quot;__main__&quot;: cluster.map(slim_call, seeds[&#39;Seed&#39;].tolist()) cluster.close() cluster.join() In this script, we iterate over seeds only, using the built-in os and multiprocessing libraries, and the popular pandas library. To install pandas, run python -m pip install pandas in a Terminal and restart Python if you have it open. We first use the pandas function read_csv() to load our seeds into a dataframe. From there we create a new pool - this is analogous to creating a new cluster with makeCluster() in R. The default setting as shown creates a pool with all available cores, however using Pool(x) where x is the number of cores you would like to use. We define a function to call slim via the os.system() command, and use our Pool’s own function map to map a list of imputs to our SLiM function. This will run the function for all values in that vector, and on as many cores available to the Pool. Using cluster.close() followed by cluster.join() is good practice, just like closing the cluster in R with stopCluster() Now lets expand this to our list of combinations like in R: from os import system from multiprocessing import Pool from itertools import product from pandas import read_csv, DataFrame from joblib import Parallel, delayed # Open the seeds file as a list seeds = read_csv(r&#39;../Inputs/seeds.csv&#39;)[&#39;Seed&#39;].to_list() # Create a list of parameters and generate unique combinations, then form a list of tuples # You can also use the pandas.write_csv() function to store this output and read it later like # we have been doing with seeds p = {&#39;param1&#39; : [0.1, 0.2, 0.3], &#39;param2&#39; : [&#39;&quot;Low&quot;&#39;, &#39;&quot;Medium&quot;&#39;, &#39;&quot;High&quot;&#39;]} keys, values = zip(*p.items()) # https://stackoverflow.com/a/61335465/13586824 combos = [dict(zip(keys, v)) for v in product(*values)] combos = DataFrame.from_dict(combos) combos = list(combos.itertuples(index=False, name=None)) # Do an operation on the pool - this is like foreach() in R # \\&quot; is an escaped string, which we need for the command line to be able to feed SLiM string parameters properly def slim_call(param1, param2): for seed in seeds: system(&#39;/path/to/slim -s {se} -d param1={p1} -d \\&quot;param2=\\&#39;{p2}\\&#39;\\&quot; ~/Desktop/example_script.slim&#39;.format(se=seed, p1=param1, p2=param2)) # Open a new &#39;pool&#39; - like makeCluster() in R cluster = Pool() if __name__ == &quot;__main__&quot;: cluster.starmap(slim_call, combos) cluster.close() cluster.join() This is a little bit more painful than in R. We construct a list of tuples of every combination of our parameters. We do this by first constructing a dictionary of the possible values for the parameters. We then use zip() to construct an iterator of tuples: basically a pairing of param1 to param2. Then we get the product of each key’s values with the other key’s values for all possible combinations in a list of dictionaries. From this list of dictionaries, we construct a pandas DataFrame, and then convert that to a list of tuples. Here, we encapsulate our SLiM call in a function which includes a for loop over seeds. We then use the Pool.starmap() function to run in parallel these SLiM runs across combinations. Indeed, this ordering could also be reversed so that the for loop contains the combinations and starmap() iterates over seeds. This might be a good idea if the number of combinations is less than the number of seeds, to leverage as much parallel power as possible and reduce wasted time. 6.6 Running SLiM via a C++ Program Before we go on, I should mention that this part is pretty much always overkill. There really isn’t much of an overhead from running SLiM in parallel via interpreted languages, so there is little reason to use any C-based language for running SLiM. It’s clunky and kind of a pain. However, it is an interesting exercise, and in the cases where you need to eke out every iota of performance during a particularly slow simulation, it might be worthwhile. This program uses an open source header library to read csv files into vectors. That can be found in the /includes/ folder, and also at the original GitHub repository. The program uses OpenMP to parallelise a for loop across multiple cores. #include &quot;includes/csv.h&quot; // https://github.com/awdeorio/csvstream #include &quot;stdlib.h&quot; #include &lt;iostream&gt; #include &lt;utility&gt; #include &lt;vector&gt; #include &lt;string&gt; #include &quot;omp.h&quot; #include &lt;map&gt; using std::vector; using std::string; #define THREAD_NUM 4 //omp_get_thread_num(); // Max CPUs on machine // Escaped &quot; and &#39; are so the command line doesn&#39;t freak out with quotations in the middle of the line for string input // Feed in a single row of the combos.csv and a single seed at a time: the parallelised for loop will do this void runSLiM(std::pair&lt;float, string&gt; combo, string seed) { float param1 = combo.first; // Access the pair&#39;s first value, which is param1 string param2 = combo.second; string callLine = &quot;/path/to/slim -s &quot; + seed + &quot; -d param1=&quot; + std::to_string(param1) + &quot; -d \\&quot;param2=\\&#39;&quot; + param2 + &quot;\\&#39;\\&quot; ~/Desktop/example_script.slim&quot;; std::system(callLine.c_str()); } int main() { // Read the seeds and combos io::CSVReader&lt;1&gt; seeds(&quot;./seeds.csv&quot;); seeds.read_header(io::ignore_extra_column, &quot;Seed&quot;); vector&lt;string&gt; vSeeds; int64_t curSeed; // For each row in the file, fill variables with that row&#39;s values while (seeds.read_row(curSeed)) { vSeeds.emplace_back(std::to_string(curSeed)); // Stick it into a vector of all seeds } io::CSVReader&lt;2&gt; combos(&quot;./combos.csv&quot;); combos.read_header(io::ignore_extra_column, &quot;param1&quot;, &quot;param2&quot;); vector&lt;std::pair&lt;float, string&gt;&gt; vCombos; float curP1; string curP2; while (combos.read_row(curP1, curP2)) { vCombos.emplace_back(curP1, curP2); // Same as above, but we are constructing a vector of pairs, where the pair is param1 and param2 } // Start of parallel processing code omp_set_num_threads(THREAD_NUM); // How many cores to use? #pragma omp parallel for collapse(2) // 2 for loops, so collapse those loops into one parallelisable structure { for (int i=0; i &lt; vSeeds.size() - 1; ++i) { for (int j=0; j &lt; vCombos.size() - 1; ++j) { runSLiM(vCombos[j], vSeeds[i]); // run SLiM with a given seed and parameter combination } } } return 0; } Most of this code is just sorting out csv files into structures that we can perform for loops on. We use openMP to parallelise a nested for loop structure and automatically dish out seed-parameter combinations to the runSLiM() function out to available cores. This code isn’t particularly nice to look at, nor is it going to work as-is for more than two parameters (std::pair will need to be another std::vector), however it could be expanded upon to be more customisable (e.g. accepting user input for filepaths). In addition, it could be part of a larger GUI app to launch parallel SLiM jobs in a more user-friendly way (coming soon?). 6.7 Writing SLiM code with parallelism in mind When it comes to writing your SLiM code, there are a few factors to keep in mind. Perhaps the most important is how you will deal with output. Will your models all write into the same output file or separate files? Each option has its pros and cons, so I’ll give descriptions of both. 6.7.1 Single file output If you want to write your output into a single file, you will need to ensure that all writeFile() calls in your script have the append flag set to T. This way, when writing to the file, SLiM will write a new line character followed by what you are writing rather than overwriting the file. Be warned that this method isn’t completely safe and can introduce you to race conditions: if two simulations try to write at the same time, which of the two will write? Will one line of results be lost, will they both be glued together and become a pain to format, or will everything go as planned? This may or may not be a problem. For example, if you are tracking evolution over long periods of time, a single missed data point isn’t going to cause you any problems realistically. However, if your data is very detailed and written multiple times during a generation, then the chances for files to be written at the same time increases. The lesson really is if you write this way, write as little as you can to minimise the risk of race conditions becoming a problem. I have written output this way through my Honours, and had no issues with overwriting or missing output. This was with 1152 concurrent simulations, with data being written 200 times per run. However, I have encountered one error since then, where a single row was appended with no newline character to another, likely due to a similar race condition. In summary, the chances of problems are low, but never zero4. 6.7.2 Multiple file output If you want to write to multiple files, then you will need to write your output into generated folders or file names based on your seed and parameter combinations. For this, I recommend defining a ‘model index’ parameter in SLiM, which is unique for each parameter combination. In fact, this parameter will be useful regardless of your output protocol, as we will see later. This can simply be set asthe row number in your parameter combination table. For example, in R: #Run SLiM foreach(i=1:nrow(df.p)) %:% foreach(j=seeds$Seed) %dopar% { slim_out &lt;- system(sprintf(&quot;/home/$USER/SLiM/slim -s %s -d param1=%f -d modelindex=%i ~/Desktop/example_script.slim&quot;, as.character(j), df.p[i,]$param1, i, intern=T)) } Here we set the SLiM constant modelindex equal to i, which is the row number of the parameter combination dataframe in our for loop. Now to name the output files, we need to combine the seed and modelindex in our SLiM script and specify a filename, like so: initialize() { if (!exists(&quot;slimgui&quot;)) { setCfgParam(&quot;seed&quot;, getSeed()); // Set the seed as a constant if we&#39;ve run from the command line } else { setSeed(asInteger(round(runif(1, 1, 2^62 - 1)))); setCfgParam(&quot;seed&quot;, getSeed()); catn(&quot;Actual seed: &quot; + asInteger(seed)); // Reset the seed to a more controlled value if we&#39;ve run from SLiMgui } setCfgParam(&quot;modelindex&quot;, 1); // Identifier for the combination of predictors used in latin hypercube: this is the row number in the lscombos.csv file setCfgParam(&quot;outputFilepath&quot;, &#39;./out_&#39; + modelindex + &#39;_&#39; + seed + &#39;_slim.csv&#39;); // Output filename/path The output filename of this simulation will be out_&lt;modelindex&gt;_&lt;seed&gt;_slim.csv. Since each simulation is writing to a different file, there is no chance of the output being overwritten5. At the end of your simulations, you can join all these files into one using the cat bash command: cat ./* &gt; slim_out.csv This will grab all files in the selected folder ./ and concatenate them together. Note that if newlines aren’t at the bottom of each file, then this won’t work. However, SLiM should automatically do this, and if not you can use the script: for file in /path/*.csv do echo &quot;&quot; &gt;&gt; &quot;$file&quot; done Which will append a newline character to the end of each file6. The main problem with this method is writing to many different files creates overhead that will slow your simulation down, and has the potential to overload the filesystem. This can cause problems for other users if you are on a HPC. My recommendation is for large simulations you should use one single large file that you constantly append to, whereas smaller runs are probably fine to do using multiple files. 6.8 Footnotes https://xkcd.com/612/↩︎ Much like the chances of your cat eating you in your sleep.↩︎ There is still a small chance of stdout losing your output if you have a massive amount of I/O that is overloading the filesystem. But just don’t do that.↩︎ https://stackoverflow.com/a/31053205/13586824↩︎ "],["running-slim-on-a-hpc-cluster.html", "7 Running SLiM on a HPC Cluster 7.1 Overview 7.2 Connecting to Tinaroo and Set-up 7.3 PBS Scripts 7.4 Multi-node jobs 7.5 Estimating Simulation Time 7.6 Other Considerations 7.7 Footnotes", " 7 Running SLiM on a HPC Cluster 7.1 Overview When running SLiM simulations, it is often necessary to run many more than you have CPU cores available on your PC or laptop. As such, even with all cores going at once, you’ll still be looking at a long time to wait to get any data back. Luckily, high-performance computing (HPC) clusters exist which allow you to use hundreds or thousands of cores concurrently. Using the same methods as the previous chapter, we can extend our parallelism to much larger systems. In this chapter, I’ll be providing some examples of how to use the University of Queensland’s (UQ’s) Tinaroo system. Most universities will have access to a similar system, and although there may be some differences in the syntax of some scripts, the concepts remain the same. 7.2 Connecting to Tinaroo and Set-up This section is mainly written for UQ students/staff wanting to access the UQ system. Readers who aren’t enrolled/employed by UQ unfortunately won’t be able to access Tinaroo. For you, I’d suggest looking into your own organisation’s HPC, and following their directions. To connect to Tinaroo, after having gained access, use the ssh command with your username: ssh &lt;username&gt;@tinaroo.rcc.uq.edu.au. You’ll be asked to confirm you trust the connection and to enter your password, and then will be put through to one of Tinaroo’s login nodes. This is a terminal just like anywhere else, and you will find yourself in your user’s home directory, which will be empty. On the login node (effectively a computer: HPCs are made of many nodes/PCs networked together), you are able to navigate directories, make files and folders, write scripts, and queue jobs. The first thing to do is install SLiM and the necessary R packages to run SLiM in parallel. Of course, you can also install the appropriate python scripts, or download necessary C++ header libraries to use if you are planning on running SLiM via those methods. To do this, we’ll start an interactive job. This will connect us to a compute node, and allow us to build SLiM without disturbing other users on the login node: qsub -I -X -A &lt;account-string-given-to-you&gt; -l select=1:ncpus=24:mem=120GB -l walltime=4:00:00 qsub is the command that queues a job to the HPC system. Tinaroo uses PBS, a job scheduler that ensures that everyone using the system is able to fairly access HPC resources for an appropriate amount of time. Other schedulers exist on other systems, so scripts on your HPC system may look slightly different to the examples given. Here we are selecting one node, with 24 cores and 120GB of RAM for a total of 4 hours. Tinaroo’s nodes contain 2 12 core processors (for 24 cores total), and 128GB of RAM, with 8GB reserved for system memory. Hence, we are asking for all of the available resources of a single node for 4 hours. The -I flag indicates the job is interactive, and the -X enables X-Forwarding, so if you would like to use RStudio, the RStudio window will appear for you on your home machine, instead of disappearing into the cybervoid7. You’ll be put into a queue for a brief period (maybe not-so-brief, depending on load), and then you will be connected to a compute node. Now we can get to work. You’ll want to make a new directory for your SLiM installation (mkdir ~/SLiM), and download and build SLiM. You will have to build SLiM manually as per the instructions in the SLiM manual, as the installation script will not work without super-user/root permissions, which you won’t have connecting to a remote HPC. This is fairly straightforward, and well explained in the manual. Note that you will not need to install SLiMgui. Some basic instructions are below. mkdir ~/SLiM cd ~/SLiM curl -O http://benhaller.com/slim/SLiM.zip unzip SLiM.zip mkdir build cd build cmake ../SLiM make slim -testEidos slim -testSLiM You’ll be left with slim and eidos executables in your build directory, which you can copy/move anywhere you like in your home directory. Having built SLiM, you’ll now want to install the parallel libraries you are using to run SLiM in parallel. Tinaroo has multiple R versions available, and it is up to you to load the one you would like. To see the list enter module spider R. Load the appropriate R version with module load R/version.number. Then, when you enter the R command, you will load a command line R interface for you to use. If this doesn’t happen, and you load into a ‘Singularity’ container, type R again and it will launch. Note that this Singularity container can affect your non-interactive jobs, so if you load a version of R with this container in your non-interactive script, you’ll need to remember to enter R a second time in your job script to launch the program properly (more on that soon). From here, install all the packages you need for your R scripts. For example: install.packages(&quot;future&quot;, &quot;doParallel&quot;, &quot;foreach&quot;) Having done this, you are ready to write some scripts. You can do this on either the compute node, or the login node, or on your local computer and transfer them across with sftp or FileZilla, which we’ll get to later. 7.3 PBS Scripts A PBS script contains the instructions that the job scheduler will use to determine what we want our job to do. It is simply a bash script with some ‘directives’ at the start to instruct the scheduler on how many resources we want to use. The interactive job line is in itself a PBS script, with some directives. Here is an example of how you might run just one SLiM run from a PBS script alone. #!/bin/bash -l #PBS -q workq #PBS -A your-account-string #PBS -N slim_eg #PBS -l walltime=24:00:00 #PBS -l select=1:ncpus=24:mem=120G cd $TMPDIR ~/SLiM/slim ~/SLiM/Scripts/Tests/Example/SLiM/slim_example.slim cat /$TMPDIR/slim_output.csv &gt;&gt; /30days/$USER/slim_output.csv As you can see, the script is split into two sections: the instructions for the scheduler, and the script/job itself. -q defines the queue you want your job put into. On Tinaroo, workq is the default, however this will be different on other systems. Generally, keeping it as the default is fine, but information on when to change your queue can be found in your HPC’s documentation, which may or may not have access restricted to university networks. -N gives your job a name for easier identification when running many jobs at once. The other flags have been explained with the interactive job. The first instruction of our job is to change our working directory to $TMPDIR, which is the local storage on the node you are working on. After each job finishes, this storage is wiped, so it is temporary and unique to every job. We do this because writing files directly to shared drives can be taxing on the system and disrupt other users. We then invoke a call to slim, running a script, and then we copy the output file to more permanent storage. To queue a job, use the qsub command: cd ~/home/$USER/SLiM/Scripts/Tests/Example/PBS qsub example_job.pbs You’ll get a message like telling you your job id number, and the PBS manager handling it. If you enter qstat you’ll get a list of all your queued and running jobs: This screen tells you the status of your job (R for running, Q for queued, X for finished), as well as its name, queue, resource requirements, etc. This command has a lot of flags that vary its output, so I’d suggest reading through them using man qstat. After a job has finished, it will leave behind two files, one for standard output, and one for standard error. If your job crashes, usually found out by if it finishes instantly, or far sooner than expected, then you should have a look at these files to see what went wrong. This are saved in the same directory that you ran the qsub command from. In our example, we cd’d to …./example/PBS, so our output will be there. The files will be named the same as your #PBS -N flag in your .pbs script, with file extensions .eXXXXX and .oXXXXX, where XXXXX is your job’s id number. Now you could put any bash code in here - including our parallel SLiM in bash example from the previous chapter (6.3). However, I’d rather use R, so let’s do that: #!/bin/bash -l #PBS -q workq #PBS -A qris-uq #PBS -N slim_eg #PBS -l walltime=24:00:00 #PBS -l select=1:ncpus=24:mem=120G cd $TMPDIR module load R/3.5.0 R --file=/home/$USER/SLiM/Scripts/Tests/Example/R/slim_sublauncher.R cat /$TMPDIR/slim_output.csv &gt;&gt; /30days/$USER/slim_output.csv Here, we do the same as before, except we load a version of R, run it with a certain file, and use that R script to handle running SLiM. Note that on Tinaroo (at time of writing) R version 3.5.0 does not have a Singularity container, so I only have to enter R once in my job script. If I was loading a version that was encapsulated by such a container, I’d need to have something like this: ... ... cd $TMPDIR module load R/3.5.0 # First R command loads the Singularity container, the second runs the script R R --file=/home/$USER/SLiM/Scripts/Tests/Example/R/slim_sublauncher.R ... ... Let’s have a look at how that script looks: ############################################################################################################## # Run SLiM in parallel ############################################################################################################## # Parallel script modified from SLiM-Extras example R script, info at # the SLiM-Extras repository at https://github.com/MesserLab/SLiM-Extras. # Environment variables USER &lt;- Sys.getenv(&#39;USER&#39;) # Parallelisation libraries library(foreach) library(doParallel) library(future) seeds &lt;- read.csv(paste0(&quot;/home/&quot;,USER,&quot;/SLiM/Scripts/Tests/Example/R/seeds.csv&quot;), header = T) combos &lt;- read.csv(paste0(&quot;/home/&quot;,USER,&quot;/SLiM/Scripts/Tests/Example/R/combos.csv&quot;), header = T) cl &lt;- makeCluster(future::availableCores()) registerDoParallel(cl) #Run SLiM foreach(i=1:nrow(combos)) %:% foreach(j=seeds$Seed) %dopar% { # Use string manipulation functions to configure the command line args, feeding from a data frame of seeds # then run SLiM with system(), slim_out &lt;- system(sprintf(&quot;/home/$USER/SLiM/slim -s %s -d param1=%f -d param2=%f -d modelindex=%i /home/$USER/SLiM/Scripts/Tests/Example/slim/slim_example.slim&quot;, as.character(j), combos[i,]$param1, combos[i,]$param2, i, intern=T)) } stopCluster(cl) This should be pretty familiar to those who read Chapter 6.4. The only real difference is we get an environment variable from the HPC, $USER, which we use to access some HPC directories. You may notice however, that we haven’t copied across the .csv files from which our seeds and combos are taken! To do this, open up FileZilla: At the top left, you’ll see a quickconnect bar. Enter the following under each option: Host: sftp://tinaroo.rcc.uq.edu.au Username: Your username Password: Your password Port: 22 When you click quickconnect, you’ll be greeted by another security message (‘do you trust to connect to this server’), and then you’ll be put through to Tinaroo. The left block is your home computer, the right block is Tinaroo’s filesystem. You can navigate on the left to your files (for example, where you have your combos.csv and seeds.csv files stored) and then on the right, navigate to where you want to put your files and drag and drop them into that folder: Now when you run your job, you’ll have 24 cores instead of the \\(&lt;10\\) of your local PC, which will make things a tad faster. But we’re still only using one node. We can access multiple nodes in a job and use many more cores at once. Which is exciting! Be excited! 7.4 Multi-node jobs There are two types of multi-node jobs on Tinaroo, and some form of both should exist on most HPCs. The first, job arrays, is like simultaneously running a whole bunch of these PBS scripts separately and having each work on a subset of your inputs. For example, your first script would handle the first 10 seeds, the next would do the next 10 seeds, etc. Ordinarily, this would be a pain because you would need a lot of scripts to manage all of that. Fortunately, the PBS system handles this for you, and you only need one extra flag to be set in your PBS script. The next system is the Embedded Nimrod system, which uses a different approach: it assigns individual cores to do jobs as they become freed, irrespective of the node they belong to. This is much more powerful for large jobs, where the amount of wasted time between one SLiM run ending and the next beginning can accumulate. 7.4.1 Job array SLiM jobs Job array jobs are very simple to set-up. A single line has to be added to the PBS Script: #!/bin/bash -l #PBS -q workq #PBS -A qris-uq #PBS -N slim_eg #PBS -J 1-4 #PBS -l walltime=24:00:00 #PBS -l select=1:ncpus=24:mem=120G cd $TMPDIR module load R/3.5.0 R --file=/home/$USER/SLiM/Scripts/Tests/Example/R/slim_sublauncher.R cat /$TMPDIR/slim_output.csv &gt;&gt; /30days/$USER/slim_output.csv #PBS -J 1-4 Is the key line here, where -J signifies a job array, and 1-4 is a vector of identifiers for each sub-job in the array. These identifiers provide a new environment variable, $PBS_ARRAY_INDEX, which can be used to identify which node is in charge of the current script. This can be loaded into R: ############################################################################################################## # Run SLiM in parallel ############################################################################################################## # Parallel script modified from SLiM-Extras example R script, info at # the SLiM-Extras repository at https://github.com/MesserLab/SLiM-Extras. # Environment variables USER &lt;- Sys.getenv(&#39;USER&#39;) ARRAY_INDEX &lt;- as.numeric(Sys.getenv(&#39;PBS_ARRAY_INDEX&#39;)) # Parallelisation libraries library(foreach) library(doParallel) library(future) seeds &lt;- read.csv(paste0(&quot;/home/&quot;,USER,&quot;/SLiM/Scripts/Tests/Example/R/seeds.csv&quot;), header = T) combos &lt;- read.csv(paste0(&quot;/home/&quot;,USER,&quot;/SLiM/Scripts/Tests/Example/R/combos.csv&quot;), header = T) # Set which runs to do according to node switch (ARR_INDEX, { combos &lt;- combos[1:5,] }, { combos &lt;- combos[6:9,] }, { combos &lt;- combos[10:13,] }, { combos &lt;- combos[14:18,] } ) cl &lt;- makeCluster(future::availableCores()) registerDoParallel(cl) #Run SLiM foreach(i=1:nrow(combos)) %:% foreach(j=seeds$Seed) %dopar% { # Use string manipulation functions to configure the command line args, feeding from a data frame of seeds # then run SLiM with system(), slim_out &lt;- system(sprintf(&quot;/home/$USER/SLiM/slim -s %s -d param1=%f -d param2=%f -d modelindex=%i /home/$USER/SLiM/Scripts/Tests/Example/slim/slim_example.slim&quot;, as.character(j), combos[i,]$param1, combos[i,]$param2, i, intern=T)) } stopCluster(cl) The R script loads in the array index, and uses that value in a switch statement to choose a certain number of rows. This way, the load of parameter combinations is spread across the nodes. The job array method is good for smaller numbers of jobs, but there is some inefficiency with queueing a lot of subjobs. An alternative is to use Tinaroo’s Embedded Nimrod system, which treats each core across many nodes as an individual device for running a process on. 7.4.2 Embedded Nimrod SLiM jobs To use Embedded Nimrod, we need to make some changes to both our R script and PBS script. In fact, our PBS isn’t even a .pbs anymore: it’s a .nsh. #!/sw7/RCC/NimrodG/embedded-1.9.0/bin/nimexec # Modified from original script by david.green@uq.edu.au # More information at: https://github.com/UQ-RCC/nimrod-embedded # # Submit this script as a PBS job script using the PBSPro qsub command. # The nimexec command will parse it into what is required. # Fix up Account String #PBS -A &lt;your-account-string-here&gt; # # Define resources: In this case, 40 nodes, 24 cores, 120GB per node #PBS -l select=40:ncpus=24:mem=120GB:ompthreads=1 #PBS -l walltime=336:00:00 # #Job name for ease of recognition #PBS -N Nim_SLiMeg # # Queue #PBS -q workq # There are additional directives for Nimrod to interpret with #NIM at the start of each line. # Tell Nimrod to use this as the shell for the job proper when it has parsed this file. #NIM shebang /bin/bash # ============================================================================= # Tell Nimrod what range of parameter values you want to use &quot;1 to 1638 step 1&quot; # ============================================================================= # The parameters for the 100 parameter combinations are rows in the input file. #NIM parameter PAR integer range from 1 to 100 step 1 # Repeat 50 times for each combination with a different SEED value #NIM parameter SEED integer range from 1 to 50 step 1 # Just checking that something did not go wrong with assignment of the J values. if [ -z &quot;${NIMROD_VAR_PAR}&quot; ]; then echo &quot;\\$NIMROD_VAR_PAR isn&#39;t set, cannot continue...&quot; exit 2 fi if [ -z &quot;${NIMROD_VAR_SEED}&quot; ]; then echo &quot;\\$NIMROD_VAR_SEED isn&#39;t set, cannot continue...&quot; exit 2 fi #Where you submit this job from will be the value of $PBS_O_WORKDIR echo &quot;PBS_O_WORKDIR is ${PBS_O_WORKDIR}&quot; #Everything you need should be located relative to PBS_O_WORKDIR, or else a full path #Set the cd to TMPDIR for writing SLiM output cd ${TMPDIR} #===================== #Modify these to suit. #===================== # Always run the entire parameter range cause nimrod can do them in any order. # See the -f test below about skipping the ones we have already done. RUNNAME=&quot;nim_sublauncher&quot; OUTFILE=&quot;${PBS_O_WORKDIR}/Outputs/TEST_${NIMROD_VAR_PAR}_${NIMROD_VAR_SEED}.txt&quot; touch &quot;${OUTFILE}&quot; if [ -f ${OUTFILE} ]; then echo &quot;Output file ${OUTFILE} already exists. Skipping this index value ${NIMROD_VAR_LS} ${NIMROD_VAR_SEED}&quot; exit 0 fi mkdir -p ./matrices/model${NIMROD_VAR_LS} RSCRIPTNAME=&quot;${PBS_O_WORKDIR}/R/${RUNNAME}.R&quot; # Get rid of any residually loaded modules and load R module purge module load R/3.5.0-gnu Rscript $RSCRIPTNAME ${NIMROD_VAR_SEED} ${NIMROD_VAR_PAR} cat /${TMPDIR}/slim_output.csv &gt;&gt; /30days/${USER}/slim_output.csv Obviously a bit more involved, so let’s go through it: The start of the script is pretty similar to a regular PBS script: you specify how many resources you want, give a job name, etc. The only difference is the addition of an ompthreads parameter in your list of resources. ompthreads is a specifier for how many cores you want each ‘subjob’ to have access to. SLiM is single-threaded, so it only needs 1 core to run, so you should set this to 1. If you were using some other program which used 4 cores, and you needed to run that in parallel, you would use ompthreads=4, and make sure that the total number of cores you request is divisible by 4. Following the familiar part, is everything else. We use #NIM parameter to set the parameters we are iterating over. In this case, we have 100 rows of an input parameter combinations list, so we are going from 1 to 100. We also have 50 seeds which we want to interate over, so we set that as a second Nimrod parameter. We have an error checking section to make sure these parameters have been set correctly, and then we go to running SLiM. First, we write empty output files (touch \"${OUTFILE}\"), which is done in case the Nimrod system misses some of the runs. If this happens (and it does), you can run the same script again, the Nimrod system will identify which runs have already been done, skip those, and only do the ones it missed the first time around. Then we go back to familiar territory, with using the bash script to sublaunch SLiM from R. This section is very similar to the PBS script, however we have defined some script names as variables rather than using direct filepaths. For example, RSCRIPTNAME=\"${PBS_O_WORKDIR}/R/${RUNNAME}.R\" is essentially the same as doing /home/$USER/SLiM/Scripts/Tests/Example/R/slim_sublauncher.R. Here, instead of R file=..., we run Rscript, which allows us to feed in environment variables directly: we can use these variables to tell R which parameter combination and seed to run for. This Rscript will run for each and every run that happens, with a different combination of NIMROD_VAR_SEED AND NIMROD_VAR_PAR. Lets have a look at how the R script works: ############################################################################################################## # Example SLiM sublauncher in R, using Tinaroo&#39;s Embedded Nimrod system # ############################################################################################################## # Parallel script modified from SLiM-Extras example R script, info at # the SLiM-Extras repository at https://github.com/MesserLab/SLiM-Extras. # Thanks to David Green (david.green@uq.edu.au) for getting this to work on Nimrod # NEED TO PROCESS 2 PARAMETERS PASSED IN REPEAT(i.e. SEED) and COMBOS ROW NUMBER args &lt;- commandArgs(trailingOnly = TRUE) if ( length(args) &lt; 2 ) { cat(&quot;Need 2 command line parameters i.e. SEED, PAR\\n&quot;) q() } row_seed &lt;- as.numeric(args[1]) row_combo &lt;- as.numeric(args[2]) # Environment variables USER &lt;- Sys.getenv(&#39;USER&#39;) # Load LHS samples - Production run is 100 samples, 50 seeds seeds &lt;- read.csv(paste0(&quot;/home/&quot;,USER,&quot;/SLiM/Scripts/Tests/Example/R/seeds.csv&quot;), header = T) combos &lt;- read.csv(paste0(&quot;/home/&quot;,USER,&quot;/SLiM/Scripts/Tests/Example/R/combos.csv&quot;), header = T) #Run SLiM, defining parameter sets according to LHC samples in command line i &lt;- row_seed j &lt;- row_combo slim_out &lt;- system(sprintf(&quot;/home/$USER/SLiM/slim -s %s -d param1=%f -d param2=%f -d modelindex=%i /home/$USER/SLiM/Scripts/Tests/Example/slim/slim_example.slim&quot;, as.character(seeds$Seed[i]), combos[j,]$param1, combos[j,]$param2, j, intern=T)) Very familiar, but there’s no for loop. R isn’t handling any parallelism in Nimrod. Each SLiM job will run its own one of these R scripts, with its own unique combination of seeds and combos. 7.5 Estimating Simulation Time When you scale up your simulations, you might notice that your job gets stuck in queue for a while. Such is the cost of shared resources, comrade. It’s important to have a look at the maximum resource allocations for your HPC and queue to make sure you aren’t stuck in an indefinite queue which will never end. On Tinaroo, the maximum wall time is 336 hours, for instance. This information can typically be found in your HPC’s user guide. A way to reduce your queue time is to estimate how much time your simulations will take to run. This way, you will have a better idea of how much walltime to request, and whether you need a large number of cores and nodes, or if you can afford to scale it back. To measure this, you need a per-simulation worst-case scenario running time. Choose the parameter values that will lead to the slowest simulation in your experiment: for example, the greatest population size in your range of values, the highest recombination rate, highest mutation rate etc. You can measure running time in SLiM using the clock() function: defineConstant(beginTime, clock()); // record the computer&#39;s time at the start of the run // The rest of your SLiM script goes here catn(&quot;Time taken: &quot; + clock() - beginTime); sim.simulationFinished(); Pretty self-explanatory, but we take the start time of the run from the end time using clock() and print out the time taken to stdout. The reason we choose the worst-case scenario rather than the average case is because typically the average can be misleading when the longest-running and shortest-running simulations are very different in time taken. In fact, in most computer experiments, the worst-case scenario is closer to the time it will take to run. If it isn’t, it is better to over-compensate than under-compensate, as the former won’t result in the job being prematurely terminated. We also account for Murphy’s law this way. Once you have a reasonable estimate of your per-run worst-case scenario, you can multiply this by your total number of runs: \\(t_{w} = t_{s}(n_{p}n_{r})\\) Where \\(t_{s}\\) is the time taken for a single run, \\(n_{p}\\) is the number of parameter combinations, and \\(n_{r}\\) is the number of replicates. This is your sequential walltime: if you were to run every simulation sequentially it would take approximately that long to do the job. That’s why we run in parallel - we need to reduce that walltime for any kind of SLiM computer experiment to be feasible. You can divide this sequential walltime by the total number of cores you are requesting to gauge how long it will actually take. Let’s do a worked example: Say I have calculated a worst-case time of 8 hours. I need to do 256 parameter combinations with 50 replicates each. Hence, my sequential walltime would be: \\(8 \\times (256 \\times 50) = 102400\\) hours. Now say I want to use 20 nodes with 24 cores each. I can divide 102400 by 480 to get 213.33 hours, about 9 days. Keep in mind that if your sequential walltime does not evenly divide into your number of cores, you will be underestimating by one of your worst-case time, since you can’t have half of a core. So in actuality, this example’s walltime would be \\(213 + 8 = 221\\) hours. If you suspect your RAM or storage might be an issue, you can do a similar exercise to calculate the maximum RAM used per simulation and the amount of storage space (although this can also be done per sample rather than per simulation to save time, assuming your samples are of equal/close to equal size for each time point). From this information, you’ll be better equipped to figure out how many nodes and cores you need to request, and also how much time the simulation will take to run. By reducing how much you’re asking for, your queue times should diminish. 7.6 Other Considerations As well as managing queuing and predicting experiment times, you will also want to keep in mind your experiment’s RAM usage. Typically SLiM simulations don’t use a great deal of memory, however this depends on a number of factors including the number of simulated individuals, and whether or not you are invoking other commands via system() which will have other associated memory costs. Remember that since each CPU core is running a separate model, your memory usage per core needs to be on average less than the total RAM per core. For example, if you are working on a HPC with 120 GB of available memory and 24 cores, that is \\(\\frac{120}{24} = 5\\) GB of memory per core, so your simulations cannot exceed an average of 5 GB per simulation. This typically isn’t a problem, but in cases where you are running R from SLiM (as mentioned in SLiM Online Workshop #14), memory usage can balloon. If you need to use R’s (or another language’s) functionality to solve a problem that SLiM doesn’t natively support, you will need to keep this in mind: if you are exceeding the memory limit, you might have to reduce the number of cores you are using per node so each running simulation has access to more memory. Another problem is in the form of getting data out: in large simulations the SLiM output can be many 100s of GBs. While not as extreme as genomic data, copying this across can still be slow. Unfortunately, since most of the data is stored in plaintext, it is difficult to compress as well. Nonetheless, using tools such as gzip, tar and zip can help with reducing the size of your output for easier transfer from the HPC’s network to your more permanent storage facility (either a local drive or another cloud storage service). 7.7 Footnotes Technical term.↩︎ "],["latin-hypercube-sampling.html", "8 Latin Hypercube Sampling 8.1 Overview 8.2 What is Latin hypercube sampling? 8.3 Generating hypercubes in R 8.4 Running SLiM with hypercube parameters 8.5 Considerations 8.6 App", " 8 Latin Hypercube Sampling 8.1 Overview When we add a lot of parameters to an experimental design, a problem appears: in order to measure each parameter’s effect across a range of parameter levels or values, you must increase the number of experiments exponentially. For example, to sample two parameters with two levels each, you need four treatments to measure them all. With three parameters with two levels each, you need eight treatment groups. By the time you’re at six parameters, you need 64 groups, and that’s just with two treatment levels each. How do you effectively sample this parameter space well then? Latin hypercube sampling offers a potential solution. 8.2 What is Latin hypercube sampling? In Latin hypercube sampling (LHC), a continuous range of each variable is specified, and combinations of these parameters are sampled together. The idea is to sample parameter combinations, which when taken together explain the total parameter space (possible values for each parameter) while minimising correlations between sampled variables (so as to avoid collinearity between factors). 8.3 Generating hypercubes in R To generate LHCs in R, you can use the DoE.wrapper package, which nicely wraps a bunch of LHC methods from LHS into an easy and user-friendly interface. # Script to generate a Latin hypercube for two parameters library(DoE.wrapper) # Sample a random 32 bit int as a seed for the LHC generation lhc_seed &lt;- sample(0:.Machine$integer.max, 1) # In the example in chapter 8, this is 1868057774 lhc &lt;- lhs.design( nruns = 512, nfactors = 2, type = &quot;maximin&quot;, factor.names = list( param1 = c(0.0, 1.0), param2 = c(100, 20000)), seed = lhc_seed ) # Diagnostics # Plot param1 against param2 to visualise any obvious gaps in sampling and correlations plot(lhc) # Return a matrix of correlations between factors cor(lhc) # Plot the histograms to check uniformity hist(lhc$param1) hist(lhc$param2) # Save the output to file write.csv(lhc, &quot;./LHC.csv&quot;) The lhs.design function creates a Latin hypercube of nruns samples, with nfactors factors/columns. You can choose a method for sampling your parameter combinations with type. I find that most of the time maximin is best suited to generate LHCs with maximised spaced between samples and minimised correlations between factors. factor.names is used to specify a list of factor names, and the minimum and maximum values to sample from. For example, here param1 will be sampled from 0 to 1. The function will output a dataframe, which you can save as a file to load and reference later. It’s a good idea to run some diagnostics on your hypercube when its done before you go to use it. You’re looking for even spacing between samples (no obvious large gaps in sampling) and very little correlation between parameters, to ensure you get a good sample of all parameters’ effects on your model. To do this, you can plot the LHC, which will give you a pairwise matrix of parameter combinations, showing you the spread of samples in the whole parameter space, which should look something like this: () You can also create histograms of each parameter to check that they are uniformly distributed with hist(), and run cor to get the correlations between parameter pairs: If you are satisfied that your hypercube looks uniformly distributed, there are no big gaps you aren’t sampling in any of your parameter combinations, and there is little correlation between parameters, then you are all good to save the hypercube with write.csv(). 8.4 Running SLiM with hypercube parameters You shouldn’t have to change any of your sublauncher script to use hypercube parameters: the LHC dataframe is formatted in the same way as any other parameter combination dataframe. Simply load the hypercube .csv instead of combos.csv, and as long as you select the right columns, you’ll be good to go. 8.5 Considerations When analysing your data, there are some considerations to make: you are treating each parameter as a continuous value, and hypercube combinations will not be nice even numbers. It’s up to you how to deal with this, but some parameters will need truncating or rounding (such as population size, which requires an integer), or ordinal variables like the distribution of size effects (normal, gamma, etc.). For something like that, you can assign cut off values for each factor level (e.g. hypercube value &lt; 0.5 is normal, &gt; 0.5 is gamma), or run your hypercube without those parameters, and repeat the experiment for each level of those ordinal variables. Note that this will exponentially increase the number of runs, as you are incorporating a level of factorial design to your experiment. In addition, individual hypercube samples can be difficult to use, as they are continuous rather than discrete levels of factors. Hence, they are better suited to be analysed as a whole rather than in sections. However, it is still possible to factorialise LHC combinations by creating levels from ranges of values. For example, the first third of your parameter range could be assigned ‘Low’, the middle third being ‘Medium’ and the top third being ‘High’. However, it’s usually not necessary to do this unless you are missing combinations of particular parameters, which can occur if you do not have enough samples (nruns in lhs.design()). 8.6 App I’ve written a little Shiny app to generate your hypercubes for you: it will show you all the diagnostic plots I have explained above, and allow you to save the file to an arbitrary location. You can run it yourself from Hypercube/app.R "],["working-with-slim-data.html", "9 Working with SLiM data 9.1 Overview 9.2 An example of saving space in custom SLiM output 9.3 Compression 9.4 Footnotes", " 9 Working with SLiM data 9.1 Overview A lot of the power of SLiM (and simulations in general) is that it is relatively cheap to keep track of a lot of data, which would be impossible in the field. SLiM’s output is extremely customisable, so anything that you keep track of during a simulation, you can output to a file. But should you? Outputting everything you are modelling as text will result in huge files which are difficult to work with, and contain irrelevant data. With more frequent sampling and more data, you’ll end up with huge plaintext files that R or Python will have a hard time reading. You could upload this to a database, but this will incur large speed penalties when trying to analyse your results. In large experiments, it is important that you only keep track of what you need to, and sample as little as you can get away with. The importance of this decreases the smaller your experiments, but it is still something to keep in mind, not only for your hard drive’s sake, but also your simulation’s speed. 9.2 An example of saving space in custom SLiM output The examples below are from the output in Chp5-1_1T.slim. Here, we take a variety of quantitative and population genetics statistics as output, saving them every so many generations: 420 Mfile = paste(sim.generation, asString(seed), modelindex, meanH, VA, phenomean, dist, w, deltaPheno, deltaw, sep=&quot;,&quot;); 421 writeFile(outName, Mfile, append = T); Mfile contains all the statistics we are measuring. We need the timepoint and model information to identify what all this data will belong to, but we don’t need every predictor variable! This is the purpose of modelindex, as explained in Chapter 5. modelindex is unique for each parameter combination, and hence describes all of those parameters in a single value. This saves a lot of space for the final output, as we are removing upwards of 3 columns (depending on how many parameters you are adjusting). You could similarly save space by saving the seed as the index of seeds in your seeds.csv file, rather than the actual 32-bit seed value, however I found this unnecessary. Not only does this save space, but it also saves processing power, as SLiM doesn’t need to fetch information on all of these parameters every time it stores output. While this is a tiny amount of time individually (a modern CPU can fetch a variable from a memory address in less than 1 nanosecond, although SLiM introduces overhead to this so it won’t be nearly as quick), in large experiments this can add up, especially if you are changing many parameters and sampling often. As mentioned in Chapter 7, as well as measuring how long your experiments will take, it can be a good idea to measure how large your output files will be, particularly if you know they are likely to be large. This will allow you to better dial-in your sampling rate according to the amount of space/RAM you have available to do analysis, and the resolution of the data you are collecting. A much larger output is also in my example script: 306 for (mut in muts) { ... 317 mutsLine = paste(sim.generation, asString(seed), modelindex, mutType, mut.id, mut.position, mut.originGeneration, mutValue, mutChi, mutFreq, mutCount, &quot;NA&quot;, sep = &quot;,&quot;); } Here, a line is created for each unique mutation existing in the population. This output quickly grows to large sizes. Hence, it is sampled less often than the other output (line 420), to ensure the simulation is fast, and that the data isn’t too big to handle. 9.3 Compression Another option is to compress your output. SLiM does have an option to compress custom output (via the writeFile(compress = T) argument), however keep in mind this will raise the memory footprint of your model. As the documentation says, \"If the compress option is used in conjunction with append==T, Eidos will buffer data to append and flush it to the file in a delayed fashion (for performance reasons), and so appended data may not be visible in the file until later – potentially not until the process ends (i.e., the end of the SLiM simulation, for example).\"8 In English, that means that SLiM holds onto the data that would be written to the file until it has time to decompress the file, add the new output line/s, and recompress the file. This probably won’t happen until the end of the simulation, however you can force this process to happen using the function flushFile(), which will stick all the ‘waiting-to-be-written’ lines of output onto the file and clear the queue. For example, if you are appending output to a file every 50 generations, and you wanted to compress this output, you could use flushFile() to keep your RAM usage under control by calling flushFile() every 500 generations or so. 9.4 Footnotes Haller, B.C. (2021) Eidos: A Simple Scripting Language, Version 3.6, pp. 71, URL: http://benhaller.com/slim/Eidos_Manual.pdf↩︎ "]]
