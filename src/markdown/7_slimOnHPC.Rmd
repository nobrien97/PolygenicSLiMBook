---
title: "Running SLiM on a HPC Cluster"
author: "Nick O'Brien"
date: "01/06/2021"
output: html_document
css: styles.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 7.1 Overview

When running SLiM simulations, it is often necessary to run many more than you have CPU cores available
on your PC or laptop. As such, even with all cores going at once, you'll still be looking at a long time 
to wait to get any data back. Luckily, high-performance computing (HPC) clusters exist which allow you
to use hundreds or thousands of cores concurrently. Using the same methods as the previous chapter, 
we can extend our parallelism to much larger systems. In this chapter, I'll be providing some examples of
how to use the University of Queensland's [Tinaroo](https://rcc.uq.edu.au/tinaroo) system. Most universities
will have access to a similar system, and although there may be some differences in the syntax of some scripts,
the concepts remain the same.


## 7.2 Connecting to Tinaroo and Set-up

To connect to Tinaroo, after having [gained access](https://rcc.uq.edu.au/tinaroo), use the ssh command with your
username: 
`ssh <username>@tinaroo.rcc.uq.edu.au`.
You'll be asked to confirm you trust the connection and to enter your password, and then will be put through to one
of Tinaroo's login nodes. This is a terminal just like anywhere else, and you will find yourself in your user's home
directory, which will be empty. On the login node (effectively a computer: HPCs are made of many nodes/PCs networked together), 
you are able to navigate directories, make files and folders, write scripts, and queue jobs. The first thing to do
is install SLiM and the necessary R packages to run SLiM in parallel.
To do this, we'll start an interactive job. This will connect us to a compute node, designed to run computations, and allow
us to build SLiM without disturbing other users on the login node:
`qsub -I -X -A <account-string-given-to-you> -l select=1:ncpus=24:mem=120GB -l walltime=4:00:00`

`qsub` is the command that queues a job to the HPC system. Tinaroo uses PBS, a job scheduler that ensures that everyone using
the system is able to fairly access appropriate resources for an appropriate amount of time. Other scehdulers also exist elsewhere,
so scripts on your HPC system may look slightly different to the examples given. Here we are selecting one node, with 
24 cores and 120GB of RAM for a total of 4 hours. Tinaroo's nodes contain 2 12 core processors (for 24 cores total), and 128GB of RAM,
with 8GB reserved for system memory. Hence, we are asking for all of the available resources of a single node for 4 hours.
The `-I` flag indicates the job is interactive, and the `-X` enables X-Forwarding, so if you would like to use RStudio, 
the RStudio window will appear for you on your home machine.

You'll be put into a queue for a brief period (maybe not-so-brief, depending on load), and then you will be connected to a compute node.
Now we can get to work. You'll want to make a new directory for your SLiM installation (`mkdir ~/SLiM`), and download and build SLiM.
You will have to build SLiM manually as per the instructions in the SLiM manual, as the installation script will not work without 
super-user/root permissions, which you won't have connecting to a remote HPC. This is fairly straightforward, and well explained 
in the manual. Note that you will not need to install SLiMgui. Some basic instructions are below.

```{bash build_slim, eval = F}
mkdir ~/SLiM
cd ~/SLiM
curl -O http://benhaller.com/slim/SLiM.zip
unzip SLiM.zip
mkdir build
cd build
cmake ../SLiM
make

slim -testEidos
slim -testSLiM
```


Having built SLiM, you'll now want to install the parallel libraries you are using to run SLiM in parallel. Tinaroo has multiple R
versions available, and it is up to you to load the one you would like. To see the list enter `module spider R`.
Load the appropriate R version with `module load R/version.number`. Then, when you enter the R command, you will load a command line
R interface for you to use. If this doesn't happen, and you load into a 'Singularity' container, type R again and it will launch.
From here, install all the packages you need for your R scripts. For example:
```{R install_dependencies, eval = F}
install.packages("future",
                 "doParallel",
                 "foreach")

```

Having done this, you are ready to write some scripts. You can do this on either the compute node, or the login node, or on your local
computer and transfer them across with `sftp` or FileZilla. 

# 7.3 PBS Scripts

A PBS script contains the instructions that the job scheduler will use to determine what we want our job to do. It is simply a 
bash script with some 'directives' at the start to instruct the scheduler on how many resources we want to use. The interactive job
line is in itself a PBS script, with some directives.
Here is an example of how you might run just one SLiM run from a PBS script alone.

```{bash PBS_eg1, eval = F}
#!/bin/bash -l
#PBS -q workq
#PBS -A your-account-string
#PBS -N slim_eg
#PBS -l walltime=24:00:00
#PBS -l select=1:ncpus=24:mem=120G

cd $TMPDIR

~/SLiM/slim ~/SLiM/Scripts/Tests/Example/SLiM/slim_example.slim

cat /$TMPDIR/slim_output.csv >> /30days/$USER/slim_output.csv

```

As you can see, the script is split into two sections: the instructions for the scheduler, and the script/job itself.
`-q` defines the queue you want your job put into. On Tinaroo, workq is the default, however this will be different on 
other systems. Generally, keeping it as the default is fine, but information on when to change your queue can be found 
in your HPC's [documentation](www2.rcc.uq.edu.au/hpc/guides/), which may or may not have access restricted to university
networks. 
`-N` gives your job a name for easier identification when running many jobs at once. The other flags have been explained
with the interactive job. 
The first instruction of our job is to change our working directory to `$TMPDIR`, which is the local storage on the node you
are working on. After each job finishes, this storage is wiped, so it is temporary and unique to every job. We do this because
writing files directly to shared drives can be taxing on the system and disrupt other users. We then invoke a call to slim,
running a script, and then we copy the output file to more permanent storage.

Now you could put any bash code in here - including our parallel SLiM in bash example from the previous chapter (6.3). However,
I'd rather use R, so let's do that:

```{bash PBS_eg2, eval = F}
#!/bin/bash -l
#PBS -q workq
#PBS -A qris-uq
#PBS -N slim_eg
#PBS -l walltime=24:00:00
#PBS -l select=1:ncpus=24:mem=120G

cd $TMPDIR

module load R/3.5.0

R --file=/home/$USER/SLiM/Scripts/Tests/Example/R/slim_sublauncher.R

cat /$TMPDIR/slim_output.csv >> /30days/$USER/slim_output.csv

```

Here, we do the same as before, except we load a version of R, run it with a certain file, and use that R script to handle running SLiM.
Let's have a look at how that script looks:

```{R PBS_Reg1, eval = F}
##############################################################################################################
#  Run SLiM in parallel
##############################################################################################################

#  Parallel script modified from SLiM-Extras example R script, info at
#  the SLiM-Extras repository at https://github.com/MesserLab/SLiM-Extras.

# Environment variables

USER <- Sys.getenv('USER')

# Parallelisation libraries 

library(foreach)
library(doParallel)
library(future)


seeds <- read.csv(paste0("/home/",USER,"/SLiM/Scripts/Tests/Example/R/seeds.csv"), header = T)
combos <- read.csv(paste0("/home/",USER,"/SLiM/Scripts/Tests/Example/R/combos.csv"), header = T)


cl <- makeCluster(future::availableCores())
registerDoParallel(cl)

#Run SLiM
foreach(i=1:nrow(combos)) %:%
  foreach(j=seeds$Seed) %dopar% {
	# Use string manipulation functions to configure the command line args, feeding from a data frame of seeds
	# then run SLiM with system(),
    	slim_out <- system(sprintf("/home/$USER/SLiM/slim -s %s -d param1=%f -d param2=%f -d modelindex=%i /home/$USER/SLiM/Scripts/Tests/Example/slim/slim_example.slim", 
        as.character(j), combos[i,]$param1, combos[i,]$param2, i, intern=T))
  }
stopCluster(cl)
```

This should be pretty familiar to those who read Chapter 6.4. The only real difference is we get an environment variable from
the HPC, `$USER`, which we use to access some HPC directories. This time, we're using 24 cores instead of between 4 and 8 though, 
which will make things a tad faster. But we're still only using one node. We can access multiple nodes in a job and use many more 
cores at once. Which is exciting! Be excited!

## 7.4 Multi-node jobs

There are two types of multi-node jobs on Tinaroo, and some form of both should exist on most HPCs. The first, job arrays,
is like simultaneously running a whole bunch of these PBS scripts separately and having each work on a subset of your inputs.
For example, your first script would handle the first 10 seeds, the next would do the next 10 seeds, etc.
Ordinarily, this would be a pain because you would need a lot of scripts to manage all of that. Fortunately, the PBS system handles
these well. The next system is the Embedded Nimrod system, which uses a different approach: it assigns individual cores to do jobs 
as they become freed, irrespective of the node they belong to. This is much more powerful for large jobs, where the amount of wasted 
time between one SLiM run ending and the next beginning can accumulate. 

### 7.4.1 Job array SLiM jobs

Job array jobs are very simple to set-up. A single line has to be added to the PBS Script:

```{bash JA_eg1, eval = F}
#!/bin/bash -l
#PBS -q workq
#PBS -A qris-uq
#PBS -N slim_eg
#PBS -J 1-4
#PBS -l walltime=24:00:00
#PBS -l select=1:ncpus=24:mem=120G

cd $TMPDIR

module load R/3.5.0

R --file=/home/$USER/SLiM/Scripts/Tests/Example/R/slim_sublauncher.R

cat /$TMPDIR/slim_output.csv >> /30days/$USER/slim_output.csv

```

`#PBS -J 1-4` Is the key line here, where `-J` signifies a job array, and `1-4` is a vector of identifiers for each sub-job in the
array. These identifiers provide a new environment variable, `$PBS_ARRAY_INDEX`, which can be used to identify which node is in charge
of the current script. This can be loaded into R:

```{R JA_Reg1, eval = F}
##############################################################################################################
#  Run SLiM in parallel
##############################################################################################################

#  Parallel script modified from SLiM-Extras example R script, info at
#  the SLiM-Extras repository at https://github.com/MesserLab/SLiM-Extras.

# Environment variables

USER <- Sys.getenv('USER')
ARRAY_INDEX <- as.numeric(Sys.getenv('PBS_ARRAY_INDEX'))

# Parallelisation libraries 

library(foreach)
library(doParallel)
library(future)


seeds <- read.csv(paste0("/home/",USER,"/SLiM/Scripts/Tests/Example/R/seeds.csv"), header = T)
combos <- read.csv(paste0("/home/",USER,"/SLiM/Scripts/Tests/Example/R/combos.csv"), header = T)

# Set which runs to do according to node

switch (ARR_INDEX,
  { combos <- combos[1:5,] },
  { combos <- combos[6:9,] },
  { combos <- combos[10:13,] },
  { combos <- combos[14:18,] }
)


cl <- makeCluster(future::availableCores())
registerDoParallel(cl)

#Run SLiM
foreach(i=1:nrow(combos)) %:%
  foreach(j=seeds$Seed) %dopar% {
	# Use string manipulation functions to configure the command line args, feeding from a data frame of seeds
	# then run SLiM with system(),
    	slim_out <- system(sprintf("/home/$USER/SLiM/slim -s %s -d param1=%f -d param2=%f -d modelindex=%i /home/$USER/SLiM/Scripts/Tests/Example/slim/slim_example.slim", 
        as.character(j), combos[i,]$param1, combos[i,]$param2, i, intern=T))
  }
stopCluster(cl)
```

The R script loads in the array index, and uses that value in a switch statement to choose a certain number of rows.
This way, the load of parameter combinations is spread across the nodes. 


### 7.4.2 Embedded Nimrod SLiM jobs