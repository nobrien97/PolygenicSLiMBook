<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Running SLiM on a HPC Cluster | Polygenic SLiMulations: Investigating polygenic adaptation with SLiM 3.</title>
  <meta name="description" content="7 Running SLiM on a HPC Cluster | Polygenic SLiMulations: Investigating polygenic adaptation with SLiM 3." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Running SLiM on a HPC Cluster | Polygenic SLiMulations: Investigating polygenic adaptation with SLiM 3." />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Running SLiM on a HPC Cluster | Polygenic SLiMulations: Investigating polygenic adaptation with SLiM 3." />
  
  
  

<meta name="author" content="Nick Oâ€™Brien" />


<meta name="date" content="2021-06-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="running-slim-in-parallel.html"/>
<link rel="next" href="latin-hypercube-sampling.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="styles.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#preface"><i class="fa fa-check"></i><b>1.1</b> Preface</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#overview"><i class="fa fa-check"></i><b>1.2</b> Overview</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#prerequisites"><i class="fa fa-check"></i><b>1.3</b> Prerequisites</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#references"><i class="fa fa-check"></i><b>1.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="installing-a-linux-environment-slim-on-the-wsl.html"><a href="installing-a-linux-environment-slim-on-the-wsl.html"><i class="fa fa-check"></i><b>2</b> Installing a Linux environment &amp; SLiM on the WSL</a>
<ul>
<li class="chapter" data-level="2.1" data-path="installing-a-linux-environment-slim-on-the-wsl.html"><a href="installing-a-linux-environment-slim-on-the-wsl.html#overview-1"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="installing-a-linux-environment-slim-on-the-wsl.html"><a href="installing-a-linux-environment-slim-on-the-wsl.html#why-linux"><i class="fa fa-check"></i><b>2.2</b> Why Linux?</a></li>
<li class="chapter" data-level="2.3" data-path="installing-a-linux-environment-slim-on-the-wsl.html"><a href="installing-a-linux-environment-slim-on-the-wsl.html#the-windows-subsystem-for-linux"><i class="fa fa-check"></i><b>2.3</b> The Windows Subsystem for Linux</a></li>
<li class="chapter" data-level="2.4" data-path="installing-a-linux-environment-slim-on-the-wsl.html"><a href="installing-a-linux-environment-slim-on-the-wsl.html#installing-wsl"><i class="fa fa-check"></i><b>2.4</b> Installing WSL</a></li>
<li class="chapter" data-level="2.5" data-path="installing-a-linux-environment-slim-on-the-wsl.html"><a href="installing-a-linux-environment-slim-on-the-wsl.html#installing-a-desktop-and-setting-up-x11"><i class="fa fa-check"></i><b>2.5</b> Installing a desktop and setting up X11</a></li>
<li class="chapter" data-level="2.6" data-path="installing-a-linux-environment-slim-on-the-wsl.html"><a href="installing-a-linux-environment-slim-on-the-wsl.html#building-slim"><i class="fa fa-check"></i><b>2.6</b> Building SLiM</a></li>
<li class="chapter" data-level="2.7" data-path="installing-a-linux-environment-slim-on-the-wsl.html"><a href="installing-a-linux-environment-slim-on-the-wsl.html#installing-other-useful-apps"><i class="fa fa-check"></i><b>2.7</b> Installing other useful apps</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="terminal-shortcuts-and-basics.html"><a href="terminal-shortcuts-and-basics.html"><i class="fa fa-check"></i><b>3</b> Terminal Shortcuts and Basics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="terminal-shortcuts-and-basics.html"><a href="terminal-shortcuts-and-basics.html#overview-2"><i class="fa fa-check"></i><b>3.1</b> Overview</a></li>
<li class="chapter" data-level="3.2" data-path="terminal-shortcuts-and-basics.html"><a href="terminal-shortcuts-and-basics.html#navigating-folders"><i class="fa fa-check"></i><b>3.2</b> Navigating folders</a></li>
<li class="chapter" data-level="3.3" data-path="terminal-shortcuts-and-basics.html"><a href="terminal-shortcuts-and-basics.html#installing-and-updating-software"><i class="fa fa-check"></i><b>3.3</b> Installing and updating software</a></li>
<li class="chapter" data-level="3.4" data-path="terminal-shortcuts-and-basics.html"><a href="terminal-shortcuts-and-basics.html#input-and-output"><i class="fa fa-check"></i><b>3.4</b> Input and output</a></li>
<li class="chapter" data-level="3.5" data-path="terminal-shortcuts-and-basics.html"><a href="terminal-shortcuts-and-basics.html#git"><i class="fa fa-check"></i><b>3.5</b> Git</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="terminal-shortcuts-and-basics.html"><a href="terminal-shortcuts-and-basics.html#git-branches-and-pull-requests"><i class="fa fa-check"></i><b>3.5.1</b> Git Branches and Pull Requests</a></li>
<li class="chapter" data-level="3.5.2" data-path="terminal-shortcuts-and-basics.html"><a href="terminal-shortcuts-and-basics.html#git-for-slim"><i class="fa fa-check"></i><b>3.5.2</b> Git for SLiM</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="a-brief-overview-of-polygenic-adaptation.html"><a href="a-brief-overview-of-polygenic-adaptation.html"><i class="fa fa-check"></i><b>4</b> A Brief Overview of Polygenic Adaptation</a>
<ul>
<li class="chapter" data-level="4.1" data-path="a-brief-overview-of-polygenic-adaptation.html"><a href="a-brief-overview-of-polygenic-adaptation.html#overview-3"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="a-brief-overview-of-polygenic-adaptation.html"><a href="a-brief-overview-of-polygenic-adaptation.html#what-is-polygenic-adaptation"><i class="fa fa-check"></i><b>4.2</b> What is polygenic adaptation?</a></li>
<li class="chapter" data-level="4.3" data-path="a-brief-overview-of-polygenic-adaptation.html"><a href="a-brief-overview-of-polygenic-adaptation.html#fishers-geometrical-model-of-adaptation"><i class="fa fa-check"></i><b>4.3</b> Fisherâ€™s Geometrical model of adaptation</a></li>
<li class="chapter" data-level="4.4" data-path="a-brief-overview-of-polygenic-adaptation.html"><a href="a-brief-overview-of-polygenic-adaptation.html#references-1"><i class="fa fa-check"></i><b>4.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="polygenic-adaptation-in-slim.html"><a href="polygenic-adaptation-in-slim.html"><i class="fa fa-check"></i><b>5</b> Polygenic Adaptation in SLiM</a>
<ul>
<li class="chapter" data-level="5.1" data-path="polygenic-adaptation-in-slim.html"><a href="polygenic-adaptation-in-slim.html#overview-4"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="polygenic-adaptation-in-slim.html"><a href="polygenic-adaptation-in-slim.html#a-single-polygenic-trait-chp5-1_1t.slim"><i class="fa fa-check"></i><b>5.2</b> A Single Polygenic Trait (Chp5-1_1T.slim)</a></li>
<li class="chapter" data-level="5.3" data-path="polygenic-adaptation-in-slim.html"><a href="polygenic-adaptation-in-slim.html#parameters"><i class="fa fa-check"></i><b>5.3</b> Parameters</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="polygenic-adaptation-in-slim.html"><a href="polygenic-adaptation-in-slim.html#ne"><i class="fa fa-check"></i><b>5.3.1</b> Ne</a></li>
<li class="chapter" data-level="5.3.2" data-path="polygenic-adaptation-in-slim.html"><a href="polygenic-adaptation-in-slim.html#del_mean-and-del_shape"><i class="fa fa-check"></i><b>5.3.2</b> del_mean and del_shape</a></li>
<li class="chapter" data-level="5.3.3" data-path="polygenic-adaptation-in-slim.html"><a href="polygenic-adaptation-in-slim.html#mutweights"><i class="fa fa-check"></i><b>5.3.3</b> mutWeights</a></li>
<li class="chapter" data-level="5.3.4" data-path="polygenic-adaptation-in-slim.html"><a href="polygenic-adaptation-in-slim.html#rwide"><i class="fa fa-check"></i><b>5.3.4</b> rwide</a></li>
<li class="chapter" data-level="5.3.5" data-path="polygenic-adaptation-in-slim.html"><a href="polygenic-adaptation-in-slim.html#genomelength"><i class="fa fa-check"></i><b>5.3.5</b> genomelength</a></li>
<li class="chapter" data-level="5.3.6" data-path="polygenic-adaptation-in-slim.html"><a href="polygenic-adaptation-in-slim.html#locimu-locisigma-and-locidist"><i class="fa fa-check"></i><b>5.3.6</b> locimu, locisigma, and locidist</a></li>
<li class="chapter" data-level="5.3.7" data-path="polygenic-adaptation-in-slim.html"><a href="polygenic-adaptation-in-slim.html#width"><i class="fa fa-check"></i><b>5.3.7</b> width</a></li>
<li class="chapter" data-level="5.3.8" data-path="polygenic-adaptation-in-slim.html"><a href="polygenic-adaptation-in-slim.html#printh"><i class="fa fa-check"></i><b>5.3.8</b> printH</a></li>
<li class="chapter" data-level="5.3.9" data-path="polygenic-adaptation-in-slim.html"><a href="polygenic-adaptation-in-slim.html#samplerate"><i class="fa fa-check"></i><b>5.3.9</b> samplerate</a></li>
<li class="chapter" data-level="5.3.10" data-path="polygenic-adaptation-in-slim.html"><a href="polygenic-adaptation-in-slim.html#modelindex"><i class="fa fa-check"></i><b>5.3.10</b> modelindex</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="running-slim-in-parallel.html"><a href="running-slim-in-parallel.html"><i class="fa fa-check"></i><b>6</b> Running SLiM in Parallel</a>
<ul>
<li class="chapter" data-level="6.1" data-path="running-slim-in-parallel.html"><a href="running-slim-in-parallel.html#overview-5"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="running-slim-in-parallel.html"><a href="running-slim-in-parallel.html#slim-at-the-command-line"><i class="fa fa-check"></i><b>6.2</b> SLiM at the Command Line</a></li>
<li class="chapter" data-level="6.3" data-path="running-slim-in-parallel.html"><a href="running-slim-in-parallel.html#running-slim-via-bash"><i class="fa fa-check"></i><b>6.3</b> Running SLiM via Bash</a></li>
<li class="chapter" data-level="6.4" data-path="running-slim-in-parallel.html"><a href="running-slim-in-parallel.html#running-slim-via-r"><i class="fa fa-check"></i><b>6.4</b> Running SLiM via R</a></li>
<li class="chapter" data-level="6.5" data-path="running-slim-in-parallel.html"><a href="running-slim-in-parallel.html#running-slim-in-python"><i class="fa fa-check"></i><b>6.5</b> Running SLiM in Python</a></li>
<li class="chapter" data-level="6.6" data-path="running-slim-in-parallel.html"><a href="running-slim-in-parallel.html#running-slim-via-a-c-program"><i class="fa fa-check"></i><b>6.6</b> Running SLiM via a C++ Program</a></li>
<li class="chapter" data-level="6.7" data-path="running-slim-in-parallel.html"><a href="running-slim-in-parallel.html#writing-slim-code-with-parallelism-in-mind"><i class="fa fa-check"></i><b>6.7</b> Writing SLiM code with parallelism in mind</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="running-slim-in-parallel.html"><a href="running-slim-in-parallel.html#single-file-output"><i class="fa fa-check"></i><b>6.7.1</b> Single file output</a></li>
<li class="chapter" data-level="6.7.2" data-path="running-slim-in-parallel.html"><a href="running-slim-in-parallel.html#multiple-file-output"><i class="fa fa-check"></i><b>6.7.2</b> Multiple file output</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="running-slim-in-parallel.html"><a href="running-slim-in-parallel.html#footnotes"><i class="fa fa-check"></i><b>6.8</b> Footnotes</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="running-slim-on-a-hpc-cluster.html"><a href="running-slim-on-a-hpc-cluster.html"><i class="fa fa-check"></i><b>7</b> Running SLiM on a HPC Cluster</a>
<ul>
<li class="chapter" data-level="7.1" data-path="running-slim-on-a-hpc-cluster.html"><a href="running-slim-on-a-hpc-cluster.html#overview-6"><i class="fa fa-check"></i><b>7.1</b> Overview</a></li>
<li class="chapter" data-level="7.2" data-path="running-slim-on-a-hpc-cluster.html"><a href="running-slim-on-a-hpc-cluster.html#connecting-to-tinaroo-and-set-up"><i class="fa fa-check"></i><b>7.2</b> Connecting to Tinaroo and Set-up</a></li>
<li class="chapter" data-level="7.3" data-path="running-slim-on-a-hpc-cluster.html"><a href="running-slim-on-a-hpc-cluster.html#pbs-scripts"><i class="fa fa-check"></i><b>7.3</b> PBS Scripts</a></li>
<li class="chapter" data-level="7.4" data-path="running-slim-on-a-hpc-cluster.html"><a href="running-slim-on-a-hpc-cluster.html#multi-node-jobs"><i class="fa fa-check"></i><b>7.4</b> Multi-node jobs</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="running-slim-on-a-hpc-cluster.html"><a href="running-slim-on-a-hpc-cluster.html#job-array-slim-jobs"><i class="fa fa-check"></i><b>7.4.1</b> Job array SLiM jobs</a></li>
<li class="chapter" data-level="7.4.2" data-path="running-slim-on-a-hpc-cluster.html"><a href="running-slim-on-a-hpc-cluster.html#embedded-nimrod-slim-jobs"><i class="fa fa-check"></i><b>7.4.2</b> Embedded Nimrod SLiM jobs</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="running-slim-on-a-hpc-cluster.html"><a href="running-slim-on-a-hpc-cluster.html#estimating-simulation-time"><i class="fa fa-check"></i><b>7.5</b> Estimating Simulation Time</a></li>
<li class="chapter" data-level="7.6" data-path="running-slim-on-a-hpc-cluster.html"><a href="running-slim-on-a-hpc-cluster.html#other-considerations"><i class="fa fa-check"></i><b>7.6</b> Other Considerations</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="latin-hypercube-sampling.html"><a href="latin-hypercube-sampling.html"><i class="fa fa-check"></i><b>8</b> Latin Hypercube Sampling</a>
<ul>
<li class="chapter" data-level="8.1" data-path="latin-hypercube-sampling.html"><a href="latin-hypercube-sampling.html#overview-7"><i class="fa fa-check"></i><b>8.1</b> Overview</a></li>
<li class="chapter" data-level="8.2" data-path="latin-hypercube-sampling.html"><a href="latin-hypercube-sampling.html#what-is-latin-hypercube-sampling"><i class="fa fa-check"></i><b>8.2</b> What is Latin hypercube sampling?</a></li>
<li class="chapter" data-level="8.3" data-path="latin-hypercube-sampling.html"><a href="latin-hypercube-sampling.html#generating-hypercubes-in-r"><i class="fa fa-check"></i><b>8.3</b> Generating hypercubes in R</a></li>
<li class="chapter" data-level="8.4" data-path="latin-hypercube-sampling.html"><a href="latin-hypercube-sampling.html#running-slim-with-hypercube-parameters"><i class="fa fa-check"></i><b>8.4</b> Running SLiM with hypercube parameters</a></li>
<li class="chapter" data-level="8.5" data-path="latin-hypercube-sampling.html"><a href="latin-hypercube-sampling.html#considerations"><i class="fa fa-check"></i><b>8.5</b> Considerations</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Polygenic SLiMulations: Investigating polygenic adaptation with SLiM 3.</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="running-slim-on-a-hpc-cluster" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Running SLiM on a HPC Cluster</h1>
<div id="overview-6" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Overview</h2>
<p>When running SLiM simulations, it is often necessary to run many more than you have CPU cores available
on your PC or laptop. As such, even with all cores going at once, youâ€™ll still be looking at a long time
to wait to get any data back. Luckily, high-performance computing (HPC) clusters exist which allow you
to use hundreds or thousands of cores concurrently. Using the same methods as the previous chapter,
we can extend our parallelism to much larger systems. In this chapter, Iâ€™ll be providing some examples of
how to use the University of Queenslandâ€™s <a href="https://rcc.uq.edu.au/tinaroo">Tinaroo</a> system. Most universities
will have access to a similar system, and although there may be some differences in the syntax of some scripts,
the concepts remain the same.</p>
</div>
<div id="connecting-to-tinaroo-and-set-up" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Connecting to Tinaroo and Set-up</h2>
<p>To connect to Tinaroo, after having <a href="https://rcc.uq.edu.au/tinaroo">gained access</a>, use the ssh command with your
username:</p>
<p><code>ssh &lt;username&gt;@tinaroo.rcc.uq.edu.au</code>.</p>
<p>Youâ€™ll be asked to confirm you trust the connection and to enter your password, and then will be put through to one
of Tinarooâ€™s login nodes. This is a terminal just like anywhere else, and you will find yourself in your userâ€™s home
directory, which will be empty. On the login node (effectively a computer: HPCs are made of many nodes/PCs networked together),
you are able to navigate directories, make files and folders, write scripts, and queue jobs. The first thing to do
is install SLiM and the necessary R packages to run SLiM in parallel.
To do this, weâ€™ll start an interactive job. This will connect us to a compute node, designed to run computations, and allow
us to build SLiM without disturbing other users on the login node:</p>
<p><code>qsub -I -X -A &lt;account-string-given-to-you&gt; -l select=1:ncpus=24:mem=120GB -l walltime=4:00:00</code></p>
<p><code>qsub</code> is the command that queues a job to the HPC system. Tinaroo uses PBS, a job scheduler that ensures that everyone using
the system is able to fairly access appropriate resources for an appropriate amount of time. Other scehdulers also exist elsewhere,
so scripts on your HPC system may look slightly different to the examples given. Here we are selecting one node, with
24 cores and 120GB of RAM for a total of 4 hours. Tinarooâ€™s nodes contain 2 12 core processors (for 24 cores total), and 128GB of RAM,
with 8GB reserved for system memory. Hence, we are asking for all of the available resources of a single node for 4 hours.
The <code>-I</code> flag indicates the job is interactive, and the <code>-X</code> enables X-Forwarding, so if you would like to use RStudio,
the RStudio window will appear for you on your home machine.</p>
<p>Youâ€™ll be put into a queue for a brief period (maybe not-so-brief, depending on load), and then you will be connected to a compute node.
Now we can get to work. Youâ€™ll want to make a new directory for your SLiM installation (<code>mkdir ~/SLiM</code>), and download and build SLiM.
You will have to build SLiM manually as per the instructions in the SLiM manual, as the installation script will not work without
super-user/root permissions, which you wonâ€™t have connecting to a remote HPC. This is fairly straightforward, and well explained
in the manual. Note that you will not need to install SLiMgui. Some basic instructions are below.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb52-1"><a href="running-slim-on-a-hpc-cluster.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> ~/SLiM</span>
<span id="cb52-2"><a href="running-slim-on-a-hpc-cluster.html#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> ~/SLiM</span>
<span id="cb52-3"><a href="running-slim-on-a-hpc-cluster.html#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> -O http://benhaller.com/slim/SLiM.zip</span>
<span id="cb52-4"><a href="running-slim-on-a-hpc-cluster.html#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="fu">unzip</span> SLiM.zip</span>
<span id="cb52-5"><a href="running-slim-on-a-hpc-cluster.html#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> build</span>
<span id="cb52-6"><a href="running-slim-on-a-hpc-cluster.html#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> build</span>
<span id="cb52-7"><a href="running-slim-on-a-hpc-cluster.html#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="fu">cmake</span> ../SLiM</span>
<span id="cb52-8"><a href="running-slim-on-a-hpc-cluster.html#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span></span>
<span id="cb52-9"><a href="running-slim-on-a-hpc-cluster.html#cb52-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-10"><a href="running-slim-on-a-hpc-cluster.html#cb52-10" aria-hidden="true" tabindex="-1"></a><span class="ex">slim</span> -testEidos</span>
<span id="cb52-11"><a href="running-slim-on-a-hpc-cluster.html#cb52-11" aria-hidden="true" tabindex="-1"></a><span class="ex">slim</span> -testSLiM</span></code></pre></div>
<p>Having built SLiM, youâ€™ll now want to install the parallel libraries you are using to run SLiM in parallel. Tinaroo has multiple R
versions available, and it is up to you to load the one you would like. To see the list enter <code>module spider R</code>.
Load the appropriate R version with <code>module load R/version.number</code>. Then, when you enter the R command, you will load a command line
R interface for you to use. If this doesnâ€™t happen, and you load into a â€˜Singularityâ€™ container, type R again and it will launch.
From here, install all the packages you need for your R scripts. For example:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="running-slim-on-a-hpc-cluster.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;future&quot;</span>,</span>
<span id="cb53-2"><a href="running-slim-on-a-hpc-cluster.html#cb53-2" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;doParallel&quot;</span>,</span>
<span id="cb53-3"><a href="running-slim-on-a-hpc-cluster.html#cb53-3" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&quot;foreach&quot;</span>)</span></code></pre></div>
<p>Having done this, you are ready to write some scripts. You can do this on either the compute node, or the login node, or on your local
computer and transfer them across with <code>sftp</code> or FileZilla, which weâ€™ll get to later.</p>
</div>
<div id="pbs-scripts" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> PBS Scripts</h2>
<p>A PBS script contains the instructions that the job scheduler will use to determine what we want our job to do. It is simply a
bash script with some â€˜directivesâ€™ at the start to instruct the scheduler on how many resources we want to use. The interactive job
line is in itself a PBS script, with some directives.
Here is an example of how you might run just one SLiM run from a PBS script alone.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb54-1"><a href="running-slim-on-a-hpc-cluster.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash -l</span></span>
<span id="cb54-2"><a href="running-slim-on-a-hpc-cluster.html#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -q workq</span></span>
<span id="cb54-3"><a href="running-slim-on-a-hpc-cluster.html#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -A your-account-string</span></span>
<span id="cb54-4"><a href="running-slim-on-a-hpc-cluster.html#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -N slim_eg</span></span>
<span id="cb54-5"><a href="running-slim-on-a-hpc-cluster.html#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -l walltime=24:00:00</span></span>
<span id="cb54-6"><a href="running-slim-on-a-hpc-cluster.html#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -l select=1:ncpus=24:mem=120G</span></span>
<span id="cb54-7"><a href="running-slim-on-a-hpc-cluster.html#cb54-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-8"><a href="running-slim-on-a-hpc-cluster.html#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> <span class="va">$TMPDIR</span></span>
<span id="cb54-9"><a href="running-slim-on-a-hpc-cluster.html#cb54-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-10"><a href="running-slim-on-a-hpc-cluster.html#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="ex">~/SLiM/slim</span> ~/SLiM/Scripts/Tests/Example/SLiM/slim_example.slim</span>
<span id="cb54-11"><a href="running-slim-on-a-hpc-cluster.html#cb54-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-12"><a href="running-slim-on-a-hpc-cluster.html#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span> /<span class="va">$TMPDIR</span>/slim_output.csv <span class="op">&gt;&gt;</span> /30days/<span class="va">$USER</span>/slim_output.csv</span></code></pre></div>
<p>As you can see, the script is split into two sections: the instructions for the scheduler, and the script/job itself.
<code>-q</code> defines the queue you want your job put into. On Tinaroo, workq is the default, however this will be different on
other systems. Generally, keeping it as the default is fine, but information on when to change your queue can be found
in your HPCâ€™s <a href="www2.rcc.uq.edu.au/hpc/guides/">documentation</a>, which may or may not have access restricted to university
networks.
<code>-N</code> gives your job a name for easier identification when running many jobs at once. The other flags have been explained
with the interactive job.
The first instruction of our job is to change our working directory to <code>$TMPDIR</code>, which is the local storage on the node you
are working on. After each job finishes, this storage is wiped, so it is temporary and unique to every job. We do this because
writing files directly to shared drives can be taxing on the system and disrupt other users. We then invoke a call to slim,
running a script, and then we copy the output file to more permanent storage.</p>
<p>To queue a job, use the <code>qsub</code> command:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb55-1"><a href="running-slim-on-a-hpc-cluster.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> ~/home/<span class="va">$USER</span>/SLiM/Scripts/Tests/Example/PBS</span>
<span id="cb55-2"><a href="running-slim-on-a-hpc-cluster.html#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="ex">qsub</span> example_job.pbs</span></code></pre></div>
<p>Youâ€™ll get a message like telling you your job id number, and the PBS manager handling it.
If you enter <code>qstat</code> youâ€™ll get a list of all your queued and running jobs:
<img src="images/chapter7/qstat.png" /></p>
<p>This screen tells you the status of your job (R for running, Q for queued, X for finished), as well as its name, queue,
resource requirements, etc.
This command has a lot of flags that vary its output, so Iâ€™d suggest reading through them using <code>man qstat</code>.</p>
<p>After a job has finished, it will leave behind two files, one for standard output, and one for standard error. If your job crashes,
usually found out by if it finishes instantly, or far sooner than expected, then you should have a look at these files to see
what went wrong. This are saved in the same directory that you ran the <code>qsub</code> command from. In our example, we <code>cd</code>â€™d to â€¦./example/PBS,
so our output will be there. The files will be named the same as your <code>#PBS -N</code> flag in your .pbs script, with file extensions .eXXXXX and .oXXXXX,
where XXXXX is your jobâ€™s id
number.</p>
<p>Now you could put any bash code in here - including our parallel SLiM in bash example from the previous chapter (6.3). However,
Iâ€™d rather use R, so letâ€™s do that:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb56-1"><a href="running-slim-on-a-hpc-cluster.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash -l</span></span>
<span id="cb56-2"><a href="running-slim-on-a-hpc-cluster.html#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -q workq</span></span>
<span id="cb56-3"><a href="running-slim-on-a-hpc-cluster.html#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -A qris-uq</span></span>
<span id="cb56-4"><a href="running-slim-on-a-hpc-cluster.html#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -N slim_eg</span></span>
<span id="cb56-5"><a href="running-slim-on-a-hpc-cluster.html#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -l walltime=24:00:00</span></span>
<span id="cb56-6"><a href="running-slim-on-a-hpc-cluster.html#cb56-6" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -l select=1:ncpus=24:mem=120G</span></span>
<span id="cb56-7"><a href="running-slim-on-a-hpc-cluster.html#cb56-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-8"><a href="running-slim-on-a-hpc-cluster.html#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> <span class="va">$TMPDIR</span></span>
<span id="cb56-9"><a href="running-slim-on-a-hpc-cluster.html#cb56-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-10"><a href="running-slim-on-a-hpc-cluster.html#cb56-10" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load R/3.5.0</span>
<span id="cb56-11"><a href="running-slim-on-a-hpc-cluster.html#cb56-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-12"><a href="running-slim-on-a-hpc-cluster.html#cb56-12" aria-hidden="true" tabindex="-1"></a><span class="ex">R</span> --file=/home/<span class="va">$USER</span>/SLiM/Scripts/Tests/Example/R/slim_sublauncher.R</span>
<span id="cb56-13"><a href="running-slim-on-a-hpc-cluster.html#cb56-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-14"><a href="running-slim-on-a-hpc-cluster.html#cb56-14" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span> /<span class="va">$TMPDIR</span>/slim_output.csv <span class="op">&gt;&gt;</span> /30days/<span class="va">$USER</span>/slim_output.csv</span></code></pre></div>
<p>Here, we do the same as before, except we load a version of R, run it with a certain file, and use that R script to handle running SLiM.
Letâ€™s have a look at how that script looks:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="running-slim-on-a-hpc-cluster.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="do">##############################################################################################################</span></span>
<span id="cb57-2"><a href="running-slim-on-a-hpc-cluster.html#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="co">#  Run SLiM in parallel</span></span>
<span id="cb57-3"><a href="running-slim-on-a-hpc-cluster.html#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="do">##############################################################################################################</span></span>
<span id="cb57-4"><a href="running-slim-on-a-hpc-cluster.html#cb57-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-5"><a href="running-slim-on-a-hpc-cluster.html#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="co">#  Parallel script modified from SLiM-Extras example R script, info at</span></span>
<span id="cb57-6"><a href="running-slim-on-a-hpc-cluster.html#cb57-6" aria-hidden="true" tabindex="-1"></a><span class="co">#  the SLiM-Extras repository at https://github.com/MesserLab/SLiM-Extras.</span></span>
<span id="cb57-7"><a href="running-slim-on-a-hpc-cluster.html#cb57-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-8"><a href="running-slim-on-a-hpc-cluster.html#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Environment variables</span></span>
<span id="cb57-9"><a href="running-slim-on-a-hpc-cluster.html#cb57-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-10"><a href="running-slim-on-a-hpc-cluster.html#cb57-10" aria-hidden="true" tabindex="-1"></a>USER <span class="ot">&lt;-</span> <span class="fu">Sys.getenv</span>(<span class="st">&#39;USER&#39;</span>)</span>
<span id="cb57-11"><a href="running-slim-on-a-hpc-cluster.html#cb57-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-12"><a href="running-slim-on-a-hpc-cluster.html#cb57-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Parallelisation libraries </span></span>
<span id="cb57-13"><a href="running-slim-on-a-hpc-cluster.html#cb57-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-14"><a href="running-slim-on-a-hpc-cluster.html#cb57-14" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(foreach)</span>
<span id="cb57-15"><a href="running-slim-on-a-hpc-cluster.html#cb57-15" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(doParallel)</span>
<span id="cb57-16"><a href="running-slim-on-a-hpc-cluster.html#cb57-16" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(future)</span>
<span id="cb57-17"><a href="running-slim-on-a-hpc-cluster.html#cb57-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-18"><a href="running-slim-on-a-hpc-cluster.html#cb57-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-19"><a href="running-slim-on-a-hpc-cluster.html#cb57-19" aria-hidden="true" tabindex="-1"></a>seeds <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="fu">paste0</span>(<span class="st">&quot;/home/&quot;</span>,USER,<span class="st">&quot;/SLiM/Scripts/Tests/Example/R/seeds.csv&quot;</span>), <span class="at">header =</span> T)</span>
<span id="cb57-20"><a href="running-slim-on-a-hpc-cluster.html#cb57-20" aria-hidden="true" tabindex="-1"></a>combos <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="fu">paste0</span>(<span class="st">&quot;/home/&quot;</span>,USER,<span class="st">&quot;/SLiM/Scripts/Tests/Example/R/combos.csv&quot;</span>), <span class="at">header =</span> T)</span>
<span id="cb57-21"><a href="running-slim-on-a-hpc-cluster.html#cb57-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-22"><a href="running-slim-on-a-hpc-cluster.html#cb57-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-23"><a href="running-slim-on-a-hpc-cluster.html#cb57-23" aria-hidden="true" tabindex="-1"></a>cl <span class="ot">&lt;-</span> <span class="fu">makeCluster</span>(future<span class="sc">::</span><span class="fu">availableCores</span>())</span>
<span id="cb57-24"><a href="running-slim-on-a-hpc-cluster.html#cb57-24" aria-hidden="true" tabindex="-1"></a><span class="fu">registerDoParallel</span>(cl)</span>
<span id="cb57-25"><a href="running-slim-on-a-hpc-cluster.html#cb57-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-26"><a href="running-slim-on-a-hpc-cluster.html#cb57-26" aria-hidden="true" tabindex="-1"></a><span class="co">#Run SLiM</span></span>
<span id="cb57-27"><a href="running-slim-on-a-hpc-cluster.html#cb57-27" aria-hidden="true" tabindex="-1"></a><span class="fu">foreach</span>(<span class="at">i=</span><span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(combos)) <span class="sc">%:%</span></span>
<span id="cb57-28"><a href="running-slim-on-a-hpc-cluster.html#cb57-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">foreach</span>(<span class="at">j=</span>seeds<span class="sc">$</span>Seed) <span class="sc">%dopar%</span> {</span>
<span id="cb57-29"><a href="running-slim-on-a-hpc-cluster.html#cb57-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use string manipulation functions to configure the command line args, feeding from a data frame of seeds</span></span>
<span id="cb57-30"><a href="running-slim-on-a-hpc-cluster.html#cb57-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># then run SLiM with system(),</span></span>
<span id="cb57-31"><a href="running-slim-on-a-hpc-cluster.html#cb57-31" aria-hidden="true" tabindex="-1"></a>        slim_out <span class="ot">&lt;-</span> <span class="fu">system</span>(<span class="fu">sprintf</span>(<span class="st">&quot;/home/$USER/SLiM/slim -s %s -d param1=%f -d param2=%f -d modelindex=%i /home/$USER/SLiM/Scripts/Tests/Example/slim/slim_example.slim&quot;</span>, </span>
<span id="cb57-32"><a href="running-slim-on-a-hpc-cluster.html#cb57-32" aria-hidden="true" tabindex="-1"></a>        <span class="fu">as.character</span>(j), combos[i,]<span class="sc">$</span>param1, combos[i,]<span class="sc">$</span>param2, i, <span class="at">intern=</span>T))</span>
<span id="cb57-33"><a href="running-slim-on-a-hpc-cluster.html#cb57-33" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb57-34"><a href="running-slim-on-a-hpc-cluster.html#cb57-34" aria-hidden="true" tabindex="-1"></a><span class="fu">stopCluster</span>(cl)</span></code></pre></div>
<p>This should be pretty familiar to those who read Chapter 6.4. The only real difference is we get an environment variable from
the HPC, <code>$USER</code>, which we use to access some HPC directories. You may notice however, that we havenâ€™t copied across the .csv files from which
our seeds and combos are taken! To do this, open up FileZilla:</p>
<p><img src="images/chapter7/filezilla.png" /></p>
<p>At the top left, youâ€™ll see a quickconnect bar. Enter the following under each option:</p>
<ul>
<li>Host: <a href="sftp://tinaroo.rcc.uq.edu.au" class="uri">sftp://tinaroo.rcc.uq.edu.au</a></li>
<li>Username: Your username</li>
<li>Password: Your password</li>
<li>Port: 22</li>
</ul>
<p>When you click quickconnect, youâ€™ll be greeted by another security message (â€˜do you trust to connect to this serverâ€™), and then youâ€™ll
be put through to Tinaroo. The left block is your home computer, the right block is Tinarooâ€™s filesystem. You can navigate on the left
to your files (for example, where you have your combos.csv and seeds.csv files stored) and then on the right, navigate to where you want
to put your files and drag and drop them into that folder:</p>
<p><img src="images/chapter7/filezilla2.png" /></p>
<p>Now when you run your job, youâ€™ll have 24 cores instead of the <span class="math inline">\(&lt;10\)</span> of your local PC, which will make things a tad faster.
But weâ€™re still only using one node. We can access multiple nodes in a job and use many more cores at once. Which is exciting! Be excited!</p>
</div>
<div id="multi-node-jobs" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Multi-node jobs</h2>
<p>There are two types of multi-node jobs on Tinaroo, and some form of both should exist on most HPCs. The first, job arrays,
is like simultaneously running a whole bunch of these PBS scripts separately and having each work on a subset of your inputs.
For example, your first script would handle the first 10 seeds, the next would do the next 10 seeds, etc.
Ordinarily, this would be a pain because you would need a lot of scripts to manage all of that. Fortunately, the PBS system handles
this for you, and you only need one extra flag to be set in your PBS script. The next system is the Embedded Nimrod system,
which uses a different approach: it assigns individual cores to do jobs as they become freed, irrespective of the node they belong to.
This is much more powerful for large jobs, where the amount of wasted time between one SLiM run ending and the next beginning can accumulate.</p>
<div id="job-array-slim-jobs" class="section level3" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Job array SLiM jobs</h3>
<p>Job array jobs are very simple to set-up. A single line has to be added to the PBS Script:</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb58-1"><a href="running-slim-on-a-hpc-cluster.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/bash -l</span></span>
<span id="cb58-2"><a href="running-slim-on-a-hpc-cluster.html#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -q workq</span></span>
<span id="cb58-3"><a href="running-slim-on-a-hpc-cluster.html#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -A qris-uq</span></span>
<span id="cb58-4"><a href="running-slim-on-a-hpc-cluster.html#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -N slim_eg</span></span>
<span id="cb58-5"><a href="running-slim-on-a-hpc-cluster.html#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -J 1-4</span></span>
<span id="cb58-6"><a href="running-slim-on-a-hpc-cluster.html#cb58-6" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -l walltime=24:00:00</span></span>
<span id="cb58-7"><a href="running-slim-on-a-hpc-cluster.html#cb58-7" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -l select=1:ncpus=24:mem=120G</span></span>
<span id="cb58-8"><a href="running-slim-on-a-hpc-cluster.html#cb58-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-9"><a href="running-slim-on-a-hpc-cluster.html#cb58-9" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> <span class="va">$TMPDIR</span></span>
<span id="cb58-10"><a href="running-slim-on-a-hpc-cluster.html#cb58-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-11"><a href="running-slim-on-a-hpc-cluster.html#cb58-11" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load R/3.5.0</span>
<span id="cb58-12"><a href="running-slim-on-a-hpc-cluster.html#cb58-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-13"><a href="running-slim-on-a-hpc-cluster.html#cb58-13" aria-hidden="true" tabindex="-1"></a><span class="ex">R</span> --file=/home/<span class="va">$USER</span>/SLiM/Scripts/Tests/Example/R/slim_sublauncher.R</span>
<span id="cb58-14"><a href="running-slim-on-a-hpc-cluster.html#cb58-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-15"><a href="running-slim-on-a-hpc-cluster.html#cb58-15" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span> /<span class="va">$TMPDIR</span>/slim_output.csv <span class="op">&gt;&gt;</span> /30days/<span class="va">$USER</span>/slim_output.csv</span></code></pre></div>
<p><code>#PBS -J 1-4</code> Is the key line here, where <code>-J</code> signifies a job array, and <code>1-4</code> is a vector of identifiers for each sub-job in the
array. These identifiers provide a new environment variable, <code>$PBS_ARRAY_INDEX</code>, which can be used to identify which node is in charge
of the current script. This can be loaded into R:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="running-slim-on-a-hpc-cluster.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="do">##############################################################################################################</span></span>
<span id="cb59-2"><a href="running-slim-on-a-hpc-cluster.html#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="co">#  Run SLiM in parallel</span></span>
<span id="cb59-3"><a href="running-slim-on-a-hpc-cluster.html#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="do">##############################################################################################################</span></span>
<span id="cb59-4"><a href="running-slim-on-a-hpc-cluster.html#cb59-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-5"><a href="running-slim-on-a-hpc-cluster.html#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="co">#  Parallel script modified from SLiM-Extras example R script, info at</span></span>
<span id="cb59-6"><a href="running-slim-on-a-hpc-cluster.html#cb59-6" aria-hidden="true" tabindex="-1"></a><span class="co">#  the SLiM-Extras repository at https://github.com/MesserLab/SLiM-Extras.</span></span>
<span id="cb59-7"><a href="running-slim-on-a-hpc-cluster.html#cb59-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-8"><a href="running-slim-on-a-hpc-cluster.html#cb59-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Environment variables</span></span>
<span id="cb59-9"><a href="running-slim-on-a-hpc-cluster.html#cb59-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-10"><a href="running-slim-on-a-hpc-cluster.html#cb59-10" aria-hidden="true" tabindex="-1"></a>USER <span class="ot">&lt;-</span> <span class="fu">Sys.getenv</span>(<span class="st">&#39;USER&#39;</span>)</span>
<span id="cb59-11"><a href="running-slim-on-a-hpc-cluster.html#cb59-11" aria-hidden="true" tabindex="-1"></a>ARRAY_INDEX <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">Sys.getenv</span>(<span class="st">&#39;PBS_ARRAY_INDEX&#39;</span>))</span>
<span id="cb59-12"><a href="running-slim-on-a-hpc-cluster.html#cb59-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-13"><a href="running-slim-on-a-hpc-cluster.html#cb59-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Parallelisation libraries </span></span>
<span id="cb59-14"><a href="running-slim-on-a-hpc-cluster.html#cb59-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-15"><a href="running-slim-on-a-hpc-cluster.html#cb59-15" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(foreach)</span>
<span id="cb59-16"><a href="running-slim-on-a-hpc-cluster.html#cb59-16" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(doParallel)</span>
<span id="cb59-17"><a href="running-slim-on-a-hpc-cluster.html#cb59-17" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(future)</span>
<span id="cb59-18"><a href="running-slim-on-a-hpc-cluster.html#cb59-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-19"><a href="running-slim-on-a-hpc-cluster.html#cb59-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-20"><a href="running-slim-on-a-hpc-cluster.html#cb59-20" aria-hidden="true" tabindex="-1"></a>seeds <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="fu">paste0</span>(<span class="st">&quot;/home/&quot;</span>,USER,<span class="st">&quot;/SLiM/Scripts/Tests/Example/R/seeds.csv&quot;</span>), <span class="at">header =</span> T)</span>
<span id="cb59-21"><a href="running-slim-on-a-hpc-cluster.html#cb59-21" aria-hidden="true" tabindex="-1"></a>combos <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="fu">paste0</span>(<span class="st">&quot;/home/&quot;</span>,USER,<span class="st">&quot;/SLiM/Scripts/Tests/Example/R/combos.csv&quot;</span>), <span class="at">header =</span> T)</span>
<span id="cb59-22"><a href="running-slim-on-a-hpc-cluster.html#cb59-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-23"><a href="running-slim-on-a-hpc-cluster.html#cb59-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Set which runs to do according to node</span></span>
<span id="cb59-24"><a href="running-slim-on-a-hpc-cluster.html#cb59-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-25"><a href="running-slim-on-a-hpc-cluster.html#cb59-25" aria-hidden="true" tabindex="-1"></a><span class="cf">switch</span> (ARR_INDEX,</span>
<span id="cb59-26"><a href="running-slim-on-a-hpc-cluster.html#cb59-26" aria-hidden="true" tabindex="-1"></a>  { combos <span class="ot">&lt;-</span> combos[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,] },</span>
<span id="cb59-27"><a href="running-slim-on-a-hpc-cluster.html#cb59-27" aria-hidden="true" tabindex="-1"></a>  { combos <span class="ot">&lt;-</span> combos[<span class="dv">6</span><span class="sc">:</span><span class="dv">9</span>,] },</span>
<span id="cb59-28"><a href="running-slim-on-a-hpc-cluster.html#cb59-28" aria-hidden="true" tabindex="-1"></a>  { combos <span class="ot">&lt;-</span> combos[<span class="dv">10</span><span class="sc">:</span><span class="dv">13</span>,] },</span>
<span id="cb59-29"><a href="running-slim-on-a-hpc-cluster.html#cb59-29" aria-hidden="true" tabindex="-1"></a>  { combos <span class="ot">&lt;-</span> combos[<span class="dv">14</span><span class="sc">:</span><span class="dv">18</span>,] }</span>
<span id="cb59-30"><a href="running-slim-on-a-hpc-cluster.html#cb59-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb59-31"><a href="running-slim-on-a-hpc-cluster.html#cb59-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-32"><a href="running-slim-on-a-hpc-cluster.html#cb59-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-33"><a href="running-slim-on-a-hpc-cluster.html#cb59-33" aria-hidden="true" tabindex="-1"></a>cl <span class="ot">&lt;-</span> <span class="fu">makeCluster</span>(future<span class="sc">::</span><span class="fu">availableCores</span>())</span>
<span id="cb59-34"><a href="running-slim-on-a-hpc-cluster.html#cb59-34" aria-hidden="true" tabindex="-1"></a><span class="fu">registerDoParallel</span>(cl)</span>
<span id="cb59-35"><a href="running-slim-on-a-hpc-cluster.html#cb59-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-36"><a href="running-slim-on-a-hpc-cluster.html#cb59-36" aria-hidden="true" tabindex="-1"></a><span class="co">#Run SLiM</span></span>
<span id="cb59-37"><a href="running-slim-on-a-hpc-cluster.html#cb59-37" aria-hidden="true" tabindex="-1"></a><span class="fu">foreach</span>(<span class="at">i=</span><span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(combos)) <span class="sc">%:%</span></span>
<span id="cb59-38"><a href="running-slim-on-a-hpc-cluster.html#cb59-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">foreach</span>(<span class="at">j=</span>seeds<span class="sc">$</span>Seed) <span class="sc">%dopar%</span> {</span>
<span id="cb59-39"><a href="running-slim-on-a-hpc-cluster.html#cb59-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use string manipulation functions to configure the command line args, feeding from a data frame of seeds</span></span>
<span id="cb59-40"><a href="running-slim-on-a-hpc-cluster.html#cb59-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># then run SLiM with system(),</span></span>
<span id="cb59-41"><a href="running-slim-on-a-hpc-cluster.html#cb59-41" aria-hidden="true" tabindex="-1"></a>        slim_out <span class="ot">&lt;-</span> <span class="fu">system</span>(<span class="fu">sprintf</span>(<span class="st">&quot;/home/$USER/SLiM/slim -s %s -d param1=%f -d param2=%f -d modelindex=%i /home/$USER/SLiM/Scripts/Tests/Example/slim/slim_example.slim&quot;</span>, </span>
<span id="cb59-42"><a href="running-slim-on-a-hpc-cluster.html#cb59-42" aria-hidden="true" tabindex="-1"></a>        <span class="fu">as.character</span>(j), combos[i,]<span class="sc">$</span>param1, combos[i,]<span class="sc">$</span>param2, i, <span class="at">intern=</span>T))</span>
<span id="cb59-43"><a href="running-slim-on-a-hpc-cluster.html#cb59-43" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb59-44"><a href="running-slim-on-a-hpc-cluster.html#cb59-44" aria-hidden="true" tabindex="-1"></a><span class="fu">stopCluster</span>(cl)</span></code></pre></div>
<p>The R script loads in the array index, and uses that value in a switch statement to choose a certain number of rows.
This way, the load of parameter combinations is spread across the nodes.</p>
<p>The job array method is good for smaller numbers of jobs, but there is some inefficiency with queueing a lot of subjobs.
An alternative is to use Tinarooâ€™s Embedded Nimrod system, which treats each core across many nodes as an individual
device for running a process on.</p>
</div>
<div id="embedded-nimrod-slim-jobs" class="section level3" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> Embedded Nimrod SLiM jobs</h3>
<p>To use Embedded Nimrod, we need to make some changes to both our R script and PBS script. In fact, our PBS isnâ€™t even a .pbs
anymore: itâ€™s a .nsh.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb60-1"><a href="running-slim-on-a-hpc-cluster.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/sw7/RCC/NimrodG/embedded-1.9.0/bin/nimexec</span></span>
<span id="cb60-2"><a href="running-slim-on-a-hpc-cluster.html#cb60-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-3"><a href="running-slim-on-a-hpc-cluster.html#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Modified from original script by david.green@uq.edu.au</span></span>
<span id="cb60-4"><a href="running-slim-on-a-hpc-cluster.html#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="co"># More information at: https://github.com/UQ-RCC/nimrod-embedded</span></span>
<span id="cb60-5"><a href="running-slim-on-a-hpc-cluster.html#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb60-6"><a href="running-slim-on-a-hpc-cluster.html#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Submit this script as a PBS job script using the PBSPro qsub command.</span></span>
<span id="cb60-7"><a href="running-slim-on-a-hpc-cluster.html#cb60-7" aria-hidden="true" tabindex="-1"></a><span class="co"># The nimexec command will parse it into what is required.</span></span>
<span id="cb60-8"><a href="running-slim-on-a-hpc-cluster.html#cb60-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-9"><a href="running-slim-on-a-hpc-cluster.html#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Fix up Account String</span></span>
<span id="cb60-10"><a href="running-slim-on-a-hpc-cluster.html#cb60-10" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -A &lt;your-account-string-here&gt;</span></span>
<span id="cb60-11"><a href="running-slim-on-a-hpc-cluster.html#cb60-11" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb60-12"><a href="running-slim-on-a-hpc-cluster.html#cb60-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Define resources: In this case, 40 nodes, 24 cores, 120GB per node</span></span>
<span id="cb60-13"><a href="running-slim-on-a-hpc-cluster.html#cb60-13" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -l select=40:ncpus=24:mem=120GB:ompthreads=1 </span></span>
<span id="cb60-14"><a href="running-slim-on-a-hpc-cluster.html#cb60-14" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -l walltime=336:00:00</span></span>
<span id="cb60-15"><a href="running-slim-on-a-hpc-cluster.html#cb60-15" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb60-16"><a href="running-slim-on-a-hpc-cluster.html#cb60-16" aria-hidden="true" tabindex="-1"></a><span class="co">#Job name for ease of recognition</span></span>
<span id="cb60-17"><a href="running-slim-on-a-hpc-cluster.html#cb60-17" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -N Nim_SLiMeg</span></span>
<span id="cb60-18"><a href="running-slim-on-a-hpc-cluster.html#cb60-18" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb60-19"><a href="running-slim-on-a-hpc-cluster.html#cb60-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Queue</span></span>
<span id="cb60-20"><a href="running-slim-on-a-hpc-cluster.html#cb60-20" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -q workq</span></span>
<span id="cb60-21"><a href="running-slim-on-a-hpc-cluster.html#cb60-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-22"><a href="running-slim-on-a-hpc-cluster.html#cb60-22" aria-hidden="true" tabindex="-1"></a><span class="co"># There are additional directives for Nimrod to interpret with #NIM at the start of each line.</span></span>
<span id="cb60-23"><a href="running-slim-on-a-hpc-cluster.html#cb60-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Tell Nimrod to use this as the shell for the job proper when it has parsed this file.</span></span>
<span id="cb60-24"><a href="running-slim-on-a-hpc-cluster.html#cb60-24" aria-hidden="true" tabindex="-1"></a><span class="co">#NIM shebang /bin/bash</span></span>
<span id="cb60-25"><a href="running-slim-on-a-hpc-cluster.html#cb60-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-26"><a href="running-slim-on-a-hpc-cluster.html#cb60-26" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb60-27"><a href="running-slim-on-a-hpc-cluster.html#cb60-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Tell Nimrod what range of parameter values you want to use &quot;1 to 1638 step 1&quot; </span></span>
<span id="cb60-28"><a href="running-slim-on-a-hpc-cluster.html#cb60-28" aria-hidden="true" tabindex="-1"></a><span class="co"># =============================================================================</span></span>
<span id="cb60-29"><a href="running-slim-on-a-hpc-cluster.html#cb60-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-30"><a href="running-slim-on-a-hpc-cluster.html#cb60-30" aria-hidden="true" tabindex="-1"></a><span class="co"># The parameters for the 100 parameter combinations are rows in the input file.</span></span>
<span id="cb60-31"><a href="running-slim-on-a-hpc-cluster.html#cb60-31" aria-hidden="true" tabindex="-1"></a><span class="co">#NIM parameter PAR integer range from 1 to 100 step 1</span></span>
<span id="cb60-32"><a href="running-slim-on-a-hpc-cluster.html#cb60-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-33"><a href="running-slim-on-a-hpc-cluster.html#cb60-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Repeat 50 times for each combination with a different SEED value</span></span>
<span id="cb60-34"><a href="running-slim-on-a-hpc-cluster.html#cb60-34" aria-hidden="true" tabindex="-1"></a><span class="co">#NIM parameter SEED integer range from 1 to 50 step 1</span></span>
<span id="cb60-35"><a href="running-slim-on-a-hpc-cluster.html#cb60-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-36"><a href="running-slim-on-a-hpc-cluster.html#cb60-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-37"><a href="running-slim-on-a-hpc-cluster.html#cb60-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Just checking that something did not go wrong with assignment of the J values.</span></span>
<span id="cb60-38"><a href="running-slim-on-a-hpc-cluster.html#cb60-38" aria-hidden="true" tabindex="-1"></a><span class="kw">if</span><span class="bu"> [</span> <span class="ot">-z</span> <span class="st">&quot;</span><span class="va">${NIMROD_VAR_PAR}</span><span class="st">&quot;</span><span class="bu"> ]</span>; <span class="kw">then</span></span>
<span id="cb60-39"><a href="running-slim-on-a-hpc-cluster.html#cb60-39" aria-hidden="true" tabindex="-1"></a>        <span class="bu">echo</span> <span class="st">&quot;</span><span class="dt">\$</span><span class="st">NIMROD_VAR_PAR isn&#39;t set, cannot continue...&quot;</span> </span>
<span id="cb60-40"><a href="running-slim-on-a-hpc-cluster.html#cb60-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">exit</span> 2</span>
<span id="cb60-41"><a href="running-slim-on-a-hpc-cluster.html#cb60-41" aria-hidden="true" tabindex="-1"></a><span class="kw">fi</span></span>
<span id="cb60-42"><a href="running-slim-on-a-hpc-cluster.html#cb60-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-43"><a href="running-slim-on-a-hpc-cluster.html#cb60-43" aria-hidden="true" tabindex="-1"></a><span class="kw">if</span><span class="bu"> [</span> <span class="ot">-z</span> <span class="st">&quot;</span><span class="va">${NIMROD_VAR_SEED}</span><span class="st">&quot;</span><span class="bu"> ]</span>; <span class="kw">then</span></span>
<span id="cb60-44"><a href="running-slim-on-a-hpc-cluster.html#cb60-44" aria-hidden="true" tabindex="-1"></a>        <span class="bu">echo</span> <span class="st">&quot;</span><span class="dt">\$</span><span class="st">NIMROD_VAR_SEED isn&#39;t set, cannot continue...&quot;</span> </span>
<span id="cb60-45"><a href="running-slim-on-a-hpc-cluster.html#cb60-45" aria-hidden="true" tabindex="-1"></a>        <span class="bu">exit</span> 2</span>
<span id="cb60-46"><a href="running-slim-on-a-hpc-cluster.html#cb60-46" aria-hidden="true" tabindex="-1"></a><span class="kw">fi</span></span>
<span id="cb60-47"><a href="running-slim-on-a-hpc-cluster.html#cb60-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-48"><a href="running-slim-on-a-hpc-cluster.html#cb60-48" aria-hidden="true" tabindex="-1"></a><span class="co">#Where you submit this job from will be the value of $PBS_O_WORKDIR</span></span>
<span id="cb60-49"><a href="running-slim-on-a-hpc-cluster.html#cb60-49" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">&quot;PBS_O_WORKDIR is </span><span class="va">${PBS_O_WORKDIR}</span><span class="st">&quot;</span> </span>
<span id="cb60-50"><a href="running-slim-on-a-hpc-cluster.html#cb60-50" aria-hidden="true" tabindex="-1"></a><span class="co">#Everything you need should be located relative to PBS_O_WORKDIR, or else a full path</span></span>
<span id="cb60-51"><a href="running-slim-on-a-hpc-cluster.html#cb60-51" aria-hidden="true" tabindex="-1"></a><span class="co">#Set the cd to TMPDIR for writing SLiM output</span></span>
<span id="cb60-52"><a href="running-slim-on-a-hpc-cluster.html#cb60-52" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> <span class="va">${TMPDIR}</span></span>
<span id="cb60-53"><a href="running-slim-on-a-hpc-cluster.html#cb60-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-54"><a href="running-slim-on-a-hpc-cluster.html#cb60-54" aria-hidden="true" tabindex="-1"></a><span class="co">#=====================</span></span>
<span id="cb60-55"><a href="running-slim-on-a-hpc-cluster.html#cb60-55" aria-hidden="true" tabindex="-1"></a><span class="co">#Modify these to suit.</span></span>
<span id="cb60-56"><a href="running-slim-on-a-hpc-cluster.html#cb60-56" aria-hidden="true" tabindex="-1"></a><span class="co">#=====================</span></span>
<span id="cb60-57"><a href="running-slim-on-a-hpc-cluster.html#cb60-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-58"><a href="running-slim-on-a-hpc-cluster.html#cb60-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Always run the entire parameter range cause nimrod can do them in any order.</span></span>
<span id="cb60-59"><a href="running-slim-on-a-hpc-cluster.html#cb60-59" aria-hidden="true" tabindex="-1"></a><span class="co"># See the -f test below about skipping the ones we have already done.</span></span>
<span id="cb60-60"><a href="running-slim-on-a-hpc-cluster.html#cb60-60" aria-hidden="true" tabindex="-1"></a><span class="va">RUNNAME=</span><span class="st">&quot;nim_sublauncher&quot;</span> </span>
<span id="cb60-61"><a href="running-slim-on-a-hpc-cluster.html#cb60-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-62"><a href="running-slim-on-a-hpc-cluster.html#cb60-62" aria-hidden="true" tabindex="-1"></a><span class="va">OUTFILE=</span><span class="st">&quot;</span><span class="va">${PBS_O_WORKDIR}</span><span class="st">/Outputs/TEST_</span><span class="va">${NIMROD_VAR_PAR}</span><span class="st">_</span><span class="va">${NIMROD_VAR_SEED}</span><span class="st">.txt&quot;</span> </span>
<span id="cb60-63"><a href="running-slim-on-a-hpc-cluster.html#cb60-63" aria-hidden="true" tabindex="-1"></a><span class="fu">touch</span> <span class="st">&quot;</span><span class="va">${OUTFILE}</span><span class="st">&quot;</span> </span>
<span id="cb60-64"><a href="running-slim-on-a-hpc-cluster.html#cb60-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-65"><a href="running-slim-on-a-hpc-cluster.html#cb60-65" aria-hidden="true" tabindex="-1"></a><span class="kw">if</span><span class="bu"> [</span> <span class="ot">-f</span> <span class="va">${OUTFILE}</span><span class="bu"> ]</span>; <span class="kw">then</span></span>
<span id="cb60-66"><a href="running-slim-on-a-hpc-cluster.html#cb60-66" aria-hidden="true" tabindex="-1"></a>  <span class="bu">echo</span> <span class="st">&quot;Output file </span><span class="va">${OUTFILE}</span><span class="st"> already exists. Skipping this index value </span><span class="va">${NIMROD_VAR_LS}</span><span class="st"> </span><span class="va">${NIMROD_VAR_SEED}</span><span class="st">&quot;</span> </span>
<span id="cb60-67"><a href="running-slim-on-a-hpc-cluster.html#cb60-67" aria-hidden="true" tabindex="-1"></a>  <span class="bu">exit</span> 0</span>
<span id="cb60-68"><a href="running-slim-on-a-hpc-cluster.html#cb60-68" aria-hidden="true" tabindex="-1"></a><span class="kw">fi</span></span>
<span id="cb60-69"><a href="running-slim-on-a-hpc-cluster.html#cb60-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-70"><a href="running-slim-on-a-hpc-cluster.html#cb60-70" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> -p ./matrices/model<span class="va">${NIMROD_VAR_LS}</span></span>
<span id="cb60-71"><a href="running-slim-on-a-hpc-cluster.html#cb60-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-72"><a href="running-slim-on-a-hpc-cluster.html#cb60-72" aria-hidden="true" tabindex="-1"></a><span class="va">RSCRIPTNAME=</span><span class="st">&quot;</span><span class="va">${PBS_O_WORKDIR}</span><span class="st">/R/</span><span class="va">${RUNNAME}</span><span class="st">.R&quot;</span> </span>
<span id="cb60-73"><a href="running-slim-on-a-hpc-cluster.html#cb60-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-74"><a href="running-slim-on-a-hpc-cluster.html#cb60-74" aria-hidden="true" tabindex="-1"></a><span class="co"># Get rid of any residually loaded modules and load R</span></span>
<span id="cb60-75"><a href="running-slim-on-a-hpc-cluster.html#cb60-75" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> purge </span>
<span id="cb60-76"><a href="running-slim-on-a-hpc-cluster.html#cb60-76" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load R/3.5.0-gnu</span>
<span id="cb60-77"><a href="running-slim-on-a-hpc-cluster.html#cb60-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-78"><a href="running-slim-on-a-hpc-cluster.html#cb60-78" aria-hidden="true" tabindex="-1"></a><span class="ex">Rscript</span> <span class="va">$RSCRIPTNAME</span> <span class="va">${NIMROD_VAR_SEED}</span> <span class="va">${NIMROD_VAR_PAR}</span></span>
<span id="cb60-79"><a href="running-slim-on-a-hpc-cluster.html#cb60-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-80"><a href="running-slim-on-a-hpc-cluster.html#cb60-80" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span> /<span class="va">${TMPDIR}</span>/slim_output.csv <span class="op">&gt;&gt;</span> /30days/<span class="va">${USER}</span>/slim_output.csv</span></code></pre></div>
<p>Obviously a bit more involved, so letâ€™s go through it:</p>
<p>The start of the script is pretty similar to a regular PBS script: you specify how many resources you want, give a job name, etc.
The only difference is the addition of an <code>ompthreads</code> parameter in your list of resources. <code>ompthreads</code> is a specifier
for how many cores you want each â€˜subjobâ€™ to have access to. SLiM is single-threaded, so it only needs 1 core to run, so
you should set this to 1. If you were using some other program which used 4 cores, and you needed to run that in parallel,
you would use <code>ompthreads=4</code>, and make sure that the total number of cores you request is divisible by 4.</p>
<p>Following the familiar part, is everything else. We use <code>#NIM parameter</code> to set the parameters we are iterating over. In this case, we
have 100 rows of an input parameter combinations list, so we are going from 1 to 100. We also have 50 seeds which we want to interate
over, so we set that as a second Nimrod parameter. We have an error checking section to make sure these parameters have been set correctly,
and then we go to running SLiM. First, we write empty output files (<code>touch "${OUTFILE}"</code>), which is done in case the Nimrod
system misses some of the runs. If this happens (and it does), you can run the same script again, the Nimrod system will identify which
runs have already been done, skip those, and only do the ones it missed the first time around. Then we go back to familiar territory, with
using the bash script to sublaunch SLiM from R.</p>
<p>This section is very similar to the PBS script, however we have defined some script names as variables rather than using direct filepaths.
For example, <code>RSCRIPTNAME="${PBS_O_WORKDIR}/R/${RUNNAME}.R"</code> is essentially the same as doing
<code>/home/$USER/SLiM/Scripts/Tests/Example/R/slim_sublauncher.R</code>.
Here, instead of <code>R file=...</code>, we run <code>Rscript</code>, which allows us to feed in environment variables directly: we can use these variables
to tell R which parameter combination and seed to run for. This Rscript will run for each and every run that happens, with a different
combination of <code>NIMROD_VAR_SEED</code> AND <code>NIMROD_VAR_PAR</code>. Lets have a look at how the R script works:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="running-slim-on-a-hpc-cluster.html#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="do">##############################################################################################################</span></span>
<span id="cb61-2"><a href="running-slim-on-a-hpc-cluster.html#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="co">#  Example SLiM sublauncher in R, using Tinaroo&#39;s Embedded Nimrod system                                     #</span></span>
<span id="cb61-3"><a href="running-slim-on-a-hpc-cluster.html#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="do">##############################################################################################################</span></span>
<span id="cb61-4"><a href="running-slim-on-a-hpc-cluster.html#cb61-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-5"><a href="running-slim-on-a-hpc-cluster.html#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="co">#  Parallel script modified from SLiM-Extras example R script, info at</span></span>
<span id="cb61-6"><a href="running-slim-on-a-hpc-cluster.html#cb61-6" aria-hidden="true" tabindex="-1"></a><span class="co">#  the SLiM-Extras repository at https://github.com/MesserLab/SLiM-Extras.</span></span>
<span id="cb61-7"><a href="running-slim-on-a-hpc-cluster.html#cb61-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-8"><a href="running-slim-on-a-hpc-cluster.html#cb61-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Thanks to David Green (david.green@uq.edu.au) for getting this to work on Nimrod</span></span>
<span id="cb61-9"><a href="running-slim-on-a-hpc-cluster.html#cb61-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-10"><a href="running-slim-on-a-hpc-cluster.html#cb61-10" aria-hidden="true" tabindex="-1"></a><span class="co"># NEED TO PROCESS 2 PARAMETERS PASSED IN REPEAT(i.e. SEED) and COMBOS ROW NUMBER</span></span>
<span id="cb61-11"><a href="running-slim-on-a-hpc-cluster.html#cb61-11" aria-hidden="true" tabindex="-1"></a>args <span class="ot">&lt;-</span> <span class="fu">commandArgs</span>(<span class="at">trailingOnly =</span> <span class="cn">TRUE</span>)</span>
<span id="cb61-12"><a href="running-slim-on-a-hpc-cluster.html#cb61-12" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> ( <span class="fu">length</span>(args) <span class="sc">&lt;</span> <span class="dv">2</span> ) {</span>
<span id="cb61-13"><a href="running-slim-on-a-hpc-cluster.html#cb61-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Need 2 command line parameters i.e. SEED, PAR</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-14"><a href="running-slim-on-a-hpc-cluster.html#cb61-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">q</span>()</span>
<span id="cb61-15"><a href="running-slim-on-a-hpc-cluster.html#cb61-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb61-16"><a href="running-slim-on-a-hpc-cluster.html#cb61-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-17"><a href="running-slim-on-a-hpc-cluster.html#cb61-17" aria-hidden="true" tabindex="-1"></a>row_seed     <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(args[<span class="dv">1</span>])</span>
<span id="cb61-18"><a href="running-slim-on-a-hpc-cluster.html#cb61-18" aria-hidden="true" tabindex="-1"></a>row_combo    <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(args[<span class="dv">2</span>])</span>
<span id="cb61-19"><a href="running-slim-on-a-hpc-cluster.html#cb61-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-20"><a href="running-slim-on-a-hpc-cluster.html#cb61-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Environment variables</span></span>
<span id="cb61-21"><a href="running-slim-on-a-hpc-cluster.html#cb61-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-22"><a href="running-slim-on-a-hpc-cluster.html#cb61-22" aria-hidden="true" tabindex="-1"></a>USER <span class="ot">&lt;-</span> <span class="fu">Sys.getenv</span>(<span class="st">&#39;USER&#39;</span>)</span>
<span id="cb61-23"><a href="running-slim-on-a-hpc-cluster.html#cb61-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-24"><a href="running-slim-on-a-hpc-cluster.html#cb61-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-25"><a href="running-slim-on-a-hpc-cluster.html#cb61-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Load LHS samples - Production run is 100 samples, 50 seeds</span></span>
<span id="cb61-26"><a href="running-slim-on-a-hpc-cluster.html#cb61-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-27"><a href="running-slim-on-a-hpc-cluster.html#cb61-27" aria-hidden="true" tabindex="-1"></a>seeds <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="fu">paste0</span>(<span class="st">&quot;/home/&quot;</span>,USER,<span class="st">&quot;/SLiM/Scripts/Tests/Example/R/seeds.csv&quot;</span>), <span class="at">header =</span> T)</span>
<span id="cb61-28"><a href="running-slim-on-a-hpc-cluster.html#cb61-28" aria-hidden="true" tabindex="-1"></a>combos <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="fu">paste0</span>(<span class="st">&quot;/home/&quot;</span>,USER,<span class="st">&quot;/SLiM/Scripts/Tests/Example/R/combos.csv&quot;</span>), <span class="at">header =</span> T)</span>
<span id="cb61-29"><a href="running-slim-on-a-hpc-cluster.html#cb61-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-30"><a href="running-slim-on-a-hpc-cluster.html#cb61-30" aria-hidden="true" tabindex="-1"></a><span class="co">#Run SLiM, defining parameter sets according to LHC samples in command line</span></span>
<span id="cb61-31"><a href="running-slim-on-a-hpc-cluster.html#cb61-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-32"><a href="running-slim-on-a-hpc-cluster.html#cb61-32" aria-hidden="true" tabindex="-1"></a><span class="co"># 8 Parameters for Aim 1 and 2, 9 for Aim 3 (wsd):</span></span>
<span id="cb61-33"><a href="running-slim-on-a-hpc-cluster.html#cb61-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Ne, rwide, pleio_cov, pleiorate, delmu, nloci, locisigma, delchr, wsd</span></span>
<span id="cb61-34"><a href="running-slim-on-a-hpc-cluster.html#cb61-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-35"><a href="running-slim-on-a-hpc-cluster.html#cb61-35" aria-hidden="true" tabindex="-1"></a>i <span class="ot">&lt;-</span> row_seed</span>
<span id="cb61-36"><a href="running-slim-on-a-hpc-cluster.html#cb61-36" aria-hidden="true" tabindex="-1"></a>j <span class="ot">&lt;-</span> row_combo</span>
<span id="cb61-37"><a href="running-slim-on-a-hpc-cluster.html#cb61-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-38"><a href="running-slim-on-a-hpc-cluster.html#cb61-38" aria-hidden="true" tabindex="-1"></a>slim_out <span class="ot">&lt;-</span> <span class="fu">system</span>(<span class="fu">sprintf</span>(<span class="st">&quot;/home/$USER/SLiM/slim -s %s -d param1=%f -d param2=%f -d modelindex=%i /home/$USER/SLiM/Scripts/Tests/Example/slim/slim_example.slim&quot;</span>, </span>
<span id="cb61-39"><a href="running-slim-on-a-hpc-cluster.html#cb61-39" aria-hidden="true" tabindex="-1"></a>        <span class="fu">as.character</span>(seeds<span class="sc">$</span>Seed[i]), combos[j,]<span class="sc">$</span>param1, combos[j,]<span class="sc">$</span>param2, j, <span class="at">intern=</span>T))</span></code></pre></div>
<p>Very familiar, but thereâ€™s no for loop. R isnâ€™t handling any parallelism in Nimrod. Each SLiM job will run its own
one of these R scripts, with its own unique combination of seeds and combos.</p>
</div>
</div>
<div id="estimating-simulation-time" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Estimating Simulation Time</h2>
<p>When you scale up your simulations, you might notice that your job gets stuck in queue for a while. Such is the cost of
shared resources, comrades. Itâ€™s important to have a look at the maximum resource allocations for your HPC and queue to
make sure you arenâ€™t stuck in an indefinite queue which will never end. On Tinaroo, the maximum wall time is 336 hours,
for instance. This information can typically be found in your HPCâ€™s user guide.</p>
<p>A way to reduce your queue time is to more accurately estimate how much time your simulations will take to run. This way,
you will have a better idea of how much walltime to request, and whether you need a large number of cores and nodes, or if
you can afford to scale it back. To measure this, you need a per-simulation worst-case scenario running time. Choose the
parameter values that will lead to the slowest simulation in your experiment: for example, the greatest population size in
your range of values, the highest recombination rate, highest mutation rate etc. You can measure running time in SLiM using the
<code>clock()</code> function:</p>
<pre class="slim"><code>defineConstant(beginTime, clock()); // record the computer&#39;s time at the start of the run 

// The rest of your SLiM script goes here

catn(&quot;Time taken: &quot; + clock() - beginTime);
sim.simulationFinished();</code></pre>
<p>Pretty self-explanatory, but we take the start time of the run from the end time using <code>clock()</code> and print out the time taken
to stdout.</p>
<p>The reason we choose the worst-case scenario rather than the average case is because typically the average can be misleading when
the longest-running and shortest-running simulations are very different in time taken. In fact, in most computer experiments,
the worst-case scenario is closer to the time it will take to run. If it isnâ€™t, it is better to over-compensate than under-compensate,
as the former wonâ€™t result in the job being prematurely terminated, meaning you would have to run it from scratch.
We also account for Murphyâ€™s law this way. Once you have a reasonable estimate of your per-run worst-case scenario, you can multiply this
by your total number of runs:
<span class="math inline">\(t_{w} = t_{s}(n_{p}n_{r})\)</span>
Where <span class="math inline">\(t_{s}\)</span> is the time taken for a single run, <span class="math inline">\(n_{p}\)</span> is the number of parameter combinations, and <span class="math inline">\(n_{r}\)</span> is the number of replicates.
This is your sequential walltime: if you were to run every simulation sequentially it would take approximately that long to do the
job. Thatâ€™s why we run in parallel - we need to reduce that walltime for any kind of SLiM computer experiment to be feasible.
You can divide this sequential walltime by the total number of cores you are requesting to gauge how long it will actually take.
Letâ€™s do a worked example:</p>
<p>Say I have calculated a worst-case time of 8 hours. I need to do 256 parameter combinations with 50 replicates each.
Hence, my sequential walltime would be:
<span class="math inline">\(8 \times (256 \times 100) = 102400\)</span> hours.
Now say I want to use 20 nodes with 24 cores each. I can divide 102400 by 480 to get 213.33 hours, about 9 days.
Keep in mind that if your sequential walltime does not evenly divide into your number of cores, you will be underestimating by
your worst-case time, since you canâ€™t have half of a core. So in actuality, this exampleâ€™s walltime would be <span class="math inline">\(213 + 8 = 221\)</span> hours.</p>
<p>If you suspect your RAM or storage might be an issue, you can do a similar exercise to calculate the maximum RAM used per simulation
and the amount of storage space (although this can also be done per sample rather than per simulation to save time, assuming your samples
are of equal/close to equal size for each time they are taken).</p>
<p>From this information, youâ€™ll be better equipped to figure out how many nodes and cores you need to request, and also how much time the
simulation will take to run.</p>
</div>
<div id="other-considerations" class="section level2" number="7.6">
<h2><span class="header-section-number">7.6</span> Other Considerations</h2>
<p>As well as managing queuing and predicting experiment times, you will also want to keep in mind your experimentâ€™s RAM usage. Typically SLiM
simulations donâ€™t use a great deal of memory, however this depends on a number of factors including the number of simulated individuals,
and whether or not you are invoking other commands via <code>system()</code> which will have other associated memory costs. Remember that since each
CPU core is running a separate model, your memory usage per core needs to be on average less than the total RAM per core. For example,
if you are working on a HPC with 120 GB of available memory and 24 cores, that is <span class="math inline">\(\frac{120}{24} = 5\)</span> GB of memory per core, so your simulations
cannot exceed an average of 5 GB per simulation. This typically isnâ€™t a problem, but in cases where you are running R from SLiM (as mentioned
in SLiM Online Workshop #14), memory usage can balloon. If you need to use Râ€™s (or another languageâ€™s) functionality to solve a problem that SLiM
doesnâ€™t natively support, you will need to keep this in mind: if you are exceeding the memory limit, you might have to reduce the number of cores
you are using per node so each running simulation has access to more memory.</p>
<p>Another problem is in the form of getting data out: in large simulations the SLiM output can be many 100s of GBs. While not as extreme as genomic
data, copying this across can still be slow. Unfortunately, since most of the data is stored in plaintext, it is difficult to compress as well.
Nonetheless, using tools such as <code>gzip</code>, <code>tar</code> and <code>zip</code> can help with reducing the size of your output for easier transfer from the HPCâ€™s network
to your more permanent storage facility (either a local drive or another cloud storage service).</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="running-slim-in-parallel.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="latin-hypercube-sampling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
